{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "# print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEC_EVENT</th>\n",
       "      <th>PAGE</th>\n",
       "      <th>CONTENT_CATEGORY</th>\n",
       "      <th>CONTENT_CATEGORY_TOP</th>\n",
       "      <th>CONTENT_CATEGORY_BOTTOM</th>\n",
       "      <th>SITE_ID</th>\n",
       "      <th>ON_SITE_SEARCH_TERM</th>\n",
       "      <th>USER_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-30 07:35:48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-03-30 07:35:52</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-03-30 07:36:11</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-03-30 07:36:16</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-03-30 07:41:38</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-03-30 07:41:42</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-03-30 07:42:01</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-03-30 07:42:05</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-03-30 07:43:43</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-03-30 07:44:14</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-03-30 07:44:37</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-03-30 07:44:58</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-02-18 13:26:45</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-02-05 14:39:37</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-03-08 10:51:34</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-03-08 10:51:37</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-03-08 10:52:05</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-03-08 10:52:06</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-03-08 10:52:06</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-03-08 10:52:12</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-01-24 12:24:29</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-01-24 12:24:49</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018-01-24 12:24:50</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018-01-24 12:24:50</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018-03-04 18:04:12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018-01-04 08:29:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018-01-04 08:42:20</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018-01-04 08:42:21</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018-01-04 08:42:38</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018-01-04 08:42:38</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936904</th>\n",
       "      <td>2018-09-17 13:41:36</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936905</th>\n",
       "      <td>2018-09-17 13:41:41</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936906</th>\n",
       "      <td>2018-09-17 13:42:01</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936907</th>\n",
       "      <td>2018-09-17 13:43:29</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936908</th>\n",
       "      <td>2018-09-17 13:48:32</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936909</th>\n",
       "      <td>2018-10-07 10:12:21</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936910</th>\n",
       "      <td>2018-10-07 10:12:32</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936911</th>\n",
       "      <td>2018-10-07 10:12:50</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936912</th>\n",
       "      <td>2018-10-07 10:12:52</td>\n",
       "      <td>190</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936913</th>\n",
       "      <td>2018-10-07 10:12:59</td>\n",
       "      <td>190</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936914</th>\n",
       "      <td>2018-10-07 10:13:01</td>\n",
       "      <td>190</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936915</th>\n",
       "      <td>2018-10-07 10:13:04</td>\n",
       "      <td>280</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936916</th>\n",
       "      <td>2018-10-07 10:13:19</td>\n",
       "      <td>190</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936917</th>\n",
       "      <td>2018-10-07 10:18:21</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936918</th>\n",
       "      <td>2018-09-17 13:40:49</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936919</th>\n",
       "      <td>2018-07-05 13:25:02</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936920</th>\n",
       "      <td>2018-07-05 13:25:22</td>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936921</th>\n",
       "      <td>2018-07-05 13:25:34</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936922</th>\n",
       "      <td>2018-07-05 13:26:34</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936923</th>\n",
       "      <td>2018-07-05 13:27:01</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936924</th>\n",
       "      <td>2018-07-05 13:28:03</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936925</th>\n",
       "      <td>2018-07-05 13:28:22</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936926</th>\n",
       "      <td>2018-07-05 13:33:24</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936927</th>\n",
       "      <td>2018-10-31 16:16:04</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936928</th>\n",
       "      <td>2018-10-31 16:17:46</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936929</th>\n",
       "      <td>2018-10-31 16:18:06</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936930</th>\n",
       "      <td>2018-10-31 16:18:35</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936931</th>\n",
       "      <td>2018-10-31 16:23:38</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936932</th>\n",
       "      <td>2018-10-16 10:53:29</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936933</th>\n",
       "      <td>2018-10-16 10:54:09</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17936934 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   FEC_EVENT  PAGE  CONTENT_CATEGORY  CONTENT_CATEGORY_TOP  \\\n",
       "0        2018-03-30 07:35:48     1                 1                     1   \n",
       "1        2018-03-30 07:35:52     2                 2                     2   \n",
       "2        2018-03-30 07:36:11     3                 2                     2   \n",
       "3        2018-03-30 07:36:16     4                 2                     2   \n",
       "4        2018-03-30 07:41:38     5                 2                     2   \n",
       "5        2018-03-30 07:41:42     2                 2                     2   \n",
       "6        2018-03-30 07:42:01     3                 2                     2   \n",
       "7        2018-03-30 07:42:05     4                 2                     2   \n",
       "8        2018-03-30 07:43:43     3                 2                     2   \n",
       "9        2018-03-30 07:44:14     6                 2                     2   \n",
       "10       2018-03-30 07:44:37     7                 2                     2   \n",
       "11       2018-03-30 07:44:58     2                 2                     2   \n",
       "12       2018-02-18 13:26:45     1                 1                     1   \n",
       "13       2018-02-05 14:39:37     8                 3                     3   \n",
       "14       2018-03-08 10:51:34     2                 2                     2   \n",
       "15       2018-03-08 10:51:37     9                 4                     2   \n",
       "16       2018-03-08 10:52:05    10                 4                     2   \n",
       "17       2018-03-08 10:52:06    11                 4                     2   \n",
       "18       2018-03-08 10:52:06    12                 4                     2   \n",
       "19       2018-03-08 10:52:12    13                 4                     2   \n",
       "20       2018-01-24 12:24:29    14                 4                     2   \n",
       "21       2018-01-24 12:24:49    10                 4                     2   \n",
       "22       2018-01-24 12:24:50    11                 4                     2   \n",
       "23       2018-01-24 12:24:50    12                 4                     2   \n",
       "24       2018-03-04 18:04:12     1                 1                     1   \n",
       "25       2018-01-04 08:29:59     1                 1                     1   \n",
       "26       2018-01-04 08:42:20     2                 2                     2   \n",
       "27       2018-01-04 08:42:21    14                 4                     2   \n",
       "28       2018-01-04 08:42:38    11                 4                     2   \n",
       "29       2018-01-04 08:42:38    12                 4                     2   \n",
       "...                      ...   ...               ...                   ...   \n",
       "17936904 2018-09-17 13:41:36     3                 2                     2   \n",
       "17936905 2018-09-17 13:41:41    64                 2                     2   \n",
       "17936906 2018-09-17 13:42:01     3                 2                     2   \n",
       "17936907 2018-09-17 13:43:29    42                 2                     2   \n",
       "17936908 2018-09-17 13:48:32     5                 2                     2   \n",
       "17936909 2018-10-07 10:12:21     2                 2                     2   \n",
       "17936910 2018-10-07 10:12:32     3                 2                     2   \n",
       "17936911 2018-10-07 10:12:50    39                 2                     2   \n",
       "17936912 2018-10-07 10:12:52   190                 2                     2   \n",
       "17936913 2018-10-07 10:12:59   190                 2                     2   \n",
       "17936914 2018-10-07 10:13:01   190                 2                     2   \n",
       "17936915 2018-10-07 10:13:04   280                 2                     2   \n",
       "17936916 2018-10-07 10:13:19   190                 2                     2   \n",
       "17936917 2018-10-07 10:18:21     5                 2                     2   \n",
       "17936918 2018-09-17 13:40:49    40                 1                     1   \n",
       "17936919 2018-07-05 13:25:02     2                 2                     2   \n",
       "17936920 2018-07-05 13:25:22    99                 2                     2   \n",
       "17936921 2018-07-05 13:25:34     2                 2                     2   \n",
       "17936922 2018-07-05 13:26:34     3                 2                     2   \n",
       "17936923 2018-07-05 13:27:01     4                 2                     2   \n",
       "17936924 2018-07-05 13:28:03     3                 2                     2   \n",
       "17936925 2018-07-05 13:28:22     4                 2                     2   \n",
       "17936926 2018-07-05 13:33:24     5                 2                     2   \n",
       "17936927 2018-10-31 16:16:04    40                 1                     1   \n",
       "17936928 2018-10-31 16:17:46     2                 2                     2   \n",
       "17936929 2018-10-31 16:18:06     3                 2                     2   \n",
       "17936930 2018-10-31 16:18:35    23                 2                     2   \n",
       "17936931 2018-10-31 16:23:38     5                 2                     2   \n",
       "17936932 2018-10-16 10:53:29     1                 1                     1   \n",
       "17936933 2018-10-16 10:54:09     2                 2                     2   \n",
       "\n",
       "          CONTENT_CATEGORY_BOTTOM  SITE_ID  ON_SITE_SEARCH_TERM  USER_ID  \n",
       "0                               1        1                    1        0  \n",
       "1                               2        2                    1        0  \n",
       "2                               2        3                    1        0  \n",
       "3                               2        3                    1        0  \n",
       "4                               2        2                    1        0  \n",
       "5                               2        2                    1        0  \n",
       "6                               2        3                    1        0  \n",
       "7                               2        3                    1        0  \n",
       "8                               2        3                    1        0  \n",
       "9                               2        3                    1        0  \n",
       "10                              2        3                    1        0  \n",
       "11                              2        2                    1        0  \n",
       "12                              1        1                    1        0  \n",
       "13                              3        1                    1        0  \n",
       "14                              2        2                    1        0  \n",
       "15                              4        2                    1        0  \n",
       "16                              4        2                    1        0  \n",
       "17                              4        2                    1        0  \n",
       "18                              4        2                    1        0  \n",
       "19                              4        2                    1        0  \n",
       "20                              4        2                    1        0  \n",
       "21                              4        2                    1        0  \n",
       "22                              4        2                    1        0  \n",
       "23                              4        2                    1        0  \n",
       "24                              1        1                    1        0  \n",
       "25                              1        1                    1        0  \n",
       "26                              2        2                    1        0  \n",
       "27                              4        2                    1        0  \n",
       "28                              4        2                    1        0  \n",
       "29                              4        2                    1        0  \n",
       "...                           ...      ...                  ...      ...  \n",
       "17936904                        2        3                    1     4639  \n",
       "17936905                        2        3                    1     4639  \n",
       "17936906                        2        3                    1     4639  \n",
       "17936907                        2        3                    1     4639  \n",
       "17936908                        2        2                    1     4639  \n",
       "17936909                        2        2                    1     4639  \n",
       "17936910                        2        3                    1     4639  \n",
       "17936911                        2        3                    1     4639  \n",
       "17936912                        2        3                    1     4639  \n",
       "17936913                        2        3                    1     4639  \n",
       "17936914                        2        3                    1     4639  \n",
       "17936915                        2        3                    1     4639  \n",
       "17936916                        2        3                    1     4639  \n",
       "17936917                        2        2                    1     4639  \n",
       "17936918                        1        1                    1     4639  \n",
       "17936919                        2        2                    1     4639  \n",
       "17936920                        2        3                    1     4639  \n",
       "17936921                        2        2                    1     4639  \n",
       "17936922                        2        3                    1     4639  \n",
       "17936923                        2        3                    1     4639  \n",
       "17936924                        2        3                    1     4639  \n",
       "17936925                        2        3                    1     4639  \n",
       "17936926                        2        2                    1     4639  \n",
       "17936927                        1        1                    1     4639  \n",
       "17936928                        2        2                    1     4639  \n",
       "17936929                        2        3                    1     4639  \n",
       "17936930                        2        3                    1     4639  \n",
       "17936931                        2        2                    1     4639  \n",
       "17936932                        1        1                    1     4639  \n",
       "17936933                        2        2                    1     4639  \n",
       "\n",
       "[17936934 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./banco-galicia-dataton-2019/pageviews/pageviews.csv\",\n",
    "                   parse_dates=[\"FEC_EVENT\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['YEAR'] = data['FEC_EVENT'].dt.year \n",
    "data['MONTH'] = data['FEC_EVENT'].dt.month \n",
    "data['DAY'] = data['FEC_EVENT'].dt.day \n",
    "data['HOUR'] = data['FEC_EVENT'].dt.hour \n",
    "# data['MINUTE'] = data['FEC_EVENT'].dt.minute\n",
    "data['WEEKDAY'] = data['FEC_EVENT'].dt.dayofweek\n",
    "weekdays = ['MONDAY', 'TUESDAY', 'WENDNESDAY', 'THURSDAY', 'FRIDAY', 'SATURDAY', 'SUNDAY']\n",
    "data['WEEKDAY'] = data.apply(lambda row: weekdays[row['WEEKDAY']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haciendo PAGE\n",
      "haciendo CONTENT_CATEGORY\n",
      "haciendo CONTENT_CATEGORY_TOP\n",
      "haciendo CONTENT_CATEGORY_BOTTOM\n",
      "haciendo SITE_ID\n",
      "haciendo ON_SITE_SEARCH_TERM\n",
      "haciendo YEAR\n",
      "haciendo MONTH\n",
      "haciendo DAY\n",
      "haciendo HOUR\n",
      "haciendo WEEKDAY\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "for c in data.drop([\"USER_ID\", \"FEC_EVENT\"], axis=1).columns:\n",
    "    print(\"haciendo\", c)\n",
    "    temp = pd.crosstab(data.USER_ID, data[c])\n",
    "    temp.columns = [c + \"_\" + str(v) for v in temp.columns]\n",
    "    X_test.append(temp.apply(lambda x: x / x.sum(), axis=1))\n",
    "X_test = pd.concat(X_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haciendo PAGE\n",
      "haciendo CONTENT_CATEGORY\n",
      "haciendo CONTENT_CATEGORY_TOP\n",
      "haciendo CONTENT_CATEGORY_BOTTOM\n",
      "haciendo SITE_ID\n",
      "haciendo ON_SITE_SEARCH_TERM\n",
      "haciendo YEAR\n",
      "haciendo MONTH\n",
      "haciendo DAY\n",
      "haciendo HOUR\n",
      "haciendo WEEKDAY\n"
     ]
    }
   ],
   "source": [
    "data = data[data.FEC_EVENT.dt.month < 10]\n",
    "X_train = []\n",
    "for c in data.drop([\"USER_ID\", \"FEC_EVENT\"], axis=1).columns:\n",
    "    print(\"haciendo\", c)\n",
    "    temp = pd.crosstab(data.USER_ID, data[c])\n",
    "    temp.columns = [c + \"_\" + str(v) for v in temp.columns]\n",
    "    X_train.append(temp.apply(lambda x: x / x.sum(), axis=1))\n",
    "X_train = pd.concat(X_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set(X_train.columns).intersection(set(X_test.columns)))\n",
    "X_train = X_train[features]\n",
    "X_test = X_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PAGE_137'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prev = pd.read_csv(\"./banco-galicia-dataton-2019/conversiones/conversiones.csv\")\n",
    "y_train = pd.Series(0, index=X_train.index)\n",
    "idx = set(y_prev[y_prev.mes >= 10].USER_ID.unique()).intersection(\n",
    "        set(X_train.index))\n",
    "y_train.loc[list(idx)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's auc: 0.821849\ttraining's binary_logloss: 0.130562\tvalid_1's auc: 0.700715\tvalid_1's binary_logloss: 0.138415\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's auc: 0.867055\ttraining's binary_logloss: 0.120285\tvalid_1's auc: 0.756623\tvalid_1's binary_logloss: 0.136268\n",
      "[3]\ttraining's auc: 0.900187\ttraining's binary_logloss: 0.112506\tvalid_1's auc: 0.739914\tvalid_1's binary_logloss: 0.135307\n",
      "[4]\ttraining's auc: 0.905376\ttraining's binary_logloss: 0.106292\tvalid_1's auc: 0.762579\tvalid_1's binary_logloss: 0.133752\n",
      "[5]\ttraining's auc: 0.923371\ttraining's binary_logloss: 0.0998041\tvalid_1's auc: 0.751461\tvalid_1's binary_logloss: 0.133529\n",
      "[6]\ttraining's auc: 0.933918\ttraining's binary_logloss: 0.0952821\tvalid_1's auc: 0.748552\tvalid_1's binary_logloss: 0.133134\n",
      "[7]\ttraining's auc: 0.937788\ttraining's binary_logloss: 0.0909564\tvalid_1's auc: 0.755553\tvalid_1's binary_logloss: 0.131659\n",
      "[8]\ttraining's auc: 0.949065\ttraining's binary_logloss: 0.0871294\tvalid_1's auc: 0.760854\tvalid_1's binary_logloss: 0.130269\n",
      "[9]\ttraining's auc: 0.95058\ttraining's binary_logloss: 0.0836751\tvalid_1's auc: 0.762126\tvalid_1's binary_logloss: 0.129325\n",
      "[10]\ttraining's auc: 0.956695\ttraining's binary_logloss: 0.080484\tvalid_1's auc: 0.760225\tvalid_1's binary_logloss: 0.12924\n",
      "[11]\ttraining's auc: 0.964243\ttraining's binary_logloss: 0.0772065\tvalid_1's auc: 0.764405\tvalid_1's binary_logloss: 0.129247\n",
      "[12]\ttraining's auc: 0.967159\ttraining's binary_logloss: 0.0742454\tvalid_1's auc: 0.767629\tvalid_1's binary_logloss: 0.128707\n",
      "[13]\ttraining's auc: 0.972132\ttraining's binary_logloss: 0.0715554\tvalid_1's auc: 0.769102\tvalid_1's binary_logloss: 0.128666\n",
      "[14]\ttraining's auc: 0.973561\ttraining's binary_logloss: 0.0691563\tvalid_1's auc: 0.766445\tvalid_1's binary_logloss: 0.129026\n",
      "[15]\ttraining's auc: 0.976093\ttraining's binary_logloss: 0.0666615\tvalid_1's auc: 0.770462\tvalid_1's binary_logloss: 0.128325\n",
      "[16]\ttraining's auc: 0.983083\ttraining's binary_logloss: 0.064276\tvalid_1's auc: 0.764116\tvalid_1's binary_logloss: 0.128208\n",
      "[17]\ttraining's auc: 0.986732\ttraining's binary_logloss: 0.0620029\tvalid_1's auc: 0.764506\tvalid_1's binary_logloss: 0.127926\n",
      "[18]\ttraining's auc: 0.988474\ttraining's binary_logloss: 0.0599274\tvalid_1's auc: 0.76574\tvalid_1's binary_logloss: 0.127696\n",
      "[19]\ttraining's auc: 0.990941\ttraining's binary_logloss: 0.0579541\tvalid_1's auc: 0.764204\tvalid_1's binary_logloss: 0.127454\n",
      "[20]\ttraining's auc: 0.992829\ttraining's binary_logloss: 0.0558655\tvalid_1's auc: 0.768032\tvalid_1's binary_logloss: 0.127556\n",
      "[21]\ttraining's auc: 0.994954\ttraining's binary_logloss: 0.0539341\tvalid_1's auc: 0.766521\tvalid_1's binary_logloss: 0.127794\n",
      "[22]\ttraining's auc: 0.996549\ttraining's binary_logloss: 0.0520389\tvalid_1's auc: 0.780485\tvalid_1's binary_logloss: 0.126674\n",
      "[23]\ttraining's auc: 0.998053\ttraining's binary_logloss: 0.0503197\tvalid_1's auc: 0.778621\tvalid_1's binary_logloss: 0.126528\n",
      "[24]\ttraining's auc: 0.998529\ttraining's binary_logloss: 0.0487723\tvalid_1's auc: 0.771419\tvalid_1's binary_logloss: 0.126995\n",
      "[25]\ttraining's auc: 0.9989\ttraining's binary_logloss: 0.0471511\tvalid_1's auc: 0.769089\tvalid_1's binary_logloss: 0.127711\n",
      "[26]\ttraining's auc: 0.999243\ttraining's binary_logloss: 0.0457091\tvalid_1's auc: 0.765639\tvalid_1's binary_logloss: 0.128219\n",
      "[27]\ttraining's auc: 0.99951\ttraining's binary_logloss: 0.0444216\tvalid_1's auc: 0.766609\tvalid_1's binary_logloss: 0.128187\n",
      "[28]\ttraining's auc: 0.999704\ttraining's binary_logloss: 0.0430952\tvalid_1's auc: 0.762907\tvalid_1's binary_logloss: 0.1286\n",
      "[29]\ttraining's auc: 0.999844\ttraining's binary_logloss: 0.0418476\tvalid_1's auc: 0.771885\tvalid_1's binary_logloss: 0.128298\n",
      "[30]\ttraining's auc: 0.999894\ttraining's binary_logloss: 0.0405433\tvalid_1's auc: 0.768661\tvalid_1's binary_logloss: 0.128954\n",
      "[31]\ttraining's auc: 0.999932\ttraining's binary_logloss: 0.0393255\tvalid_1's auc: 0.766697\tvalid_1's binary_logloss: 0.129208\n",
      "[32]\ttraining's auc: 0.999948\ttraining's binary_logloss: 0.0381972\tvalid_1's auc: 0.769215\tvalid_1's binary_logloss: 0.129462\n",
      "Early stopping, best iteration is:\n",
      "[22]\ttraining's auc: 0.996549\ttraining's binary_logloss: 0.0520389\tvalid_1's auc: 0.780485\tvalid_1's binary_logloss: 0.126674\n",
      "[1]\ttraining's auc: 0.839598\ttraining's binary_logloss: 0.127214\tvalid_1's auc: 0.70059\tvalid_1's binary_logloss: 0.168251\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's auc: 0.869142\ttraining's binary_logloss: 0.116343\tvalid_1's auc: 0.788926\tvalid_1's binary_logloss: 0.164483\n",
      "[3]\ttraining's auc: 0.89595\ttraining's binary_logloss: 0.108855\tvalid_1's auc: 0.791266\tvalid_1's binary_logloss: 0.161795\n",
      "[4]\ttraining's auc: 0.913288\ttraining's binary_logloss: 0.102637\tvalid_1's auc: 0.802914\tvalid_1's binary_logloss: 0.159515\n",
      "[5]\ttraining's auc: 0.922605\ttraining's binary_logloss: 0.097221\tvalid_1's auc: 0.807058\tvalid_1's binary_logloss: 0.157798\n",
      "[6]\ttraining's auc: 0.932358\ttraining's binary_logloss: 0.0922084\tvalid_1's auc: 0.814438\tvalid_1's binary_logloss: 0.156004\n",
      "[7]\ttraining's auc: 0.940739\ttraining's binary_logloss: 0.0879155\tvalid_1's auc: 0.812443\tvalid_1's binary_logloss: 0.155552\n",
      "[8]\ttraining's auc: 0.943268\ttraining's binary_logloss: 0.0840276\tvalid_1's auc: 0.816033\tvalid_1's binary_logloss: 0.154544\n",
      "[9]\ttraining's auc: 0.955064\ttraining's binary_logloss: 0.0805724\tvalid_1's auc: 0.815995\tvalid_1's binary_logloss: 0.153803\n",
      "[10]\ttraining's auc: 0.960368\ttraining's binary_logloss: 0.0774437\tvalid_1's auc: 0.812462\tvalid_1's binary_logloss: 0.153703\n",
      "[11]\ttraining's auc: 0.970441\ttraining's binary_logloss: 0.0742885\tvalid_1's auc: 0.813235\tvalid_1's binary_logloss: 0.153684\n",
      "[12]\ttraining's auc: 0.9762\ttraining's binary_logloss: 0.0713717\tvalid_1's auc: 0.810018\tvalid_1's binary_logloss: 0.153776\n",
      "[13]\ttraining's auc: 0.980979\ttraining's binary_logloss: 0.0684375\tvalid_1's auc: 0.803134\tvalid_1's binary_logloss: 0.153032\n",
      "[14]\ttraining's auc: 0.982152\ttraining's binary_logloss: 0.0657168\tvalid_1's auc: 0.808041\tvalid_1's binary_logloss: 0.152526\n",
      "[15]\ttraining's auc: 0.988138\ttraining's binary_logloss: 0.0631419\tvalid_1's auc: 0.809626\tvalid_1's binary_logloss: 0.151849\n",
      "[16]\ttraining's auc: 0.988889\ttraining's binary_logloss: 0.0610838\tvalid_1's auc: 0.808347\tvalid_1's binary_logloss: 0.151576\n",
      "[17]\ttraining's auc: 0.989361\ttraining's binary_logloss: 0.0590291\tvalid_1's auc: 0.808986\tvalid_1's binary_logloss: 0.151498\n",
      "[18]\ttraining's auc: 0.992216\ttraining's binary_logloss: 0.0571149\tvalid_1's auc: 0.810314\tvalid_1's binary_logloss: 0.151476\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's auc: 0.943268\ttraining's binary_logloss: 0.0840276\tvalid_1's auc: 0.816033\tvalid_1's binary_logloss: 0.154544\n",
      "[1]\ttraining's auc: 0.840262\ttraining's binary_logloss: 0.132413\tvalid_1's auc: 0.785169\tvalid_1's binary_logloss: 0.120148\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's auc: 0.863003\ttraining's binary_logloss: 0.122434\tvalid_1's auc: 0.803016\tvalid_1's binary_logloss: 0.11626\n",
      "[3]\ttraining's auc: 0.880517\ttraining's binary_logloss: 0.114514\tvalid_1's auc: 0.823236\tvalid_1's binary_logloss: 0.114194\n",
      "[4]\ttraining's auc: 0.900103\ttraining's binary_logloss: 0.107947\tvalid_1's auc: 0.821547\tvalid_1's binary_logloss: 0.112804\n",
      "[5]\ttraining's auc: 0.915846\ttraining's binary_logloss: 0.102653\tvalid_1's auc: 0.80255\tvalid_1's binary_logloss: 0.112254\n",
      "[6]\ttraining's auc: 0.931309\ttraining's binary_logloss: 0.0975409\tvalid_1's auc: 0.815142\tvalid_1's binary_logloss: 0.111609\n",
      "[7]\ttraining's auc: 0.937581\ttraining's binary_logloss: 0.0933117\tvalid_1's auc: 0.816278\tvalid_1's binary_logloss: 0.110609\n",
      "[8]\ttraining's auc: 0.941665\ttraining's binary_logloss: 0.0892131\tvalid_1's auc: 0.817005\tvalid_1's binary_logloss: 0.109631\n",
      "[9]\ttraining's auc: 0.950223\ttraining's binary_logloss: 0.085708\tvalid_1's auc: 0.819844\tvalid_1's binary_logloss: 0.108623\n",
      "[10]\ttraining's auc: 0.954088\ttraining's binary_logloss: 0.0823679\tvalid_1's auc: 0.827676\tvalid_1's binary_logloss: 0.107759\n",
      "[11]\ttraining's auc: 0.960414\ttraining's binary_logloss: 0.0793682\tvalid_1's auc: 0.82705\tvalid_1's binary_logloss: 0.107665\n",
      "[12]\ttraining's auc: 0.961575\ttraining's binary_logloss: 0.0763818\tvalid_1's auc: 0.829612\tvalid_1's binary_logloss: 0.106999\n",
      "[13]\ttraining's auc: 0.968168\ttraining's binary_logloss: 0.0734487\tvalid_1's auc: 0.837938\tvalid_1's binary_logloss: 0.106535\n",
      "[14]\ttraining's auc: 0.976343\ttraining's binary_logloss: 0.0710324\tvalid_1's auc: 0.839015\tvalid_1's binary_logloss: 0.10585\n",
      "[15]\ttraining's auc: 0.980909\ttraining's binary_logloss: 0.0685103\tvalid_1's auc: 0.841854\tvalid_1's binary_logloss: 0.105193\n",
      "[16]\ttraining's auc: 0.982058\ttraining's binary_logloss: 0.0663732\tvalid_1's auc: 0.838127\tvalid_1's binary_logloss: 0.105399\n",
      "[17]\ttraining's auc: 0.986398\ttraining's binary_logloss: 0.0638107\tvalid_1's auc: 0.837167\tvalid_1's binary_logloss: 0.105087\n",
      "[18]\ttraining's auc: 0.987377\ttraining's binary_logloss: 0.0616819\tvalid_1's auc: 0.838055\tvalid_1's binary_logloss: 0.104865\n",
      "[19]\ttraining's auc: 0.990994\ttraining's binary_logloss: 0.0595834\tvalid_1's auc: 0.838375\tvalid_1's binary_logloss: 0.104669\n",
      "[20]\ttraining's auc: 0.992371\ttraining's binary_logloss: 0.0576554\tvalid_1's auc: 0.839569\tvalid_1's binary_logloss: 0.104257\n",
      "[21]\ttraining's auc: 0.994647\ttraining's binary_logloss: 0.0557264\tvalid_1's auc: 0.838462\tvalid_1's binary_logloss: 0.104151\n",
      "[22]\ttraining's auc: 0.996721\ttraining's binary_logloss: 0.053969\tvalid_1's auc: 0.841403\tvalid_1's binary_logloss: 0.103606\n",
      "[23]\ttraining's auc: 0.997807\ttraining's binary_logloss: 0.0521874\tvalid_1's auc: 0.845799\tvalid_1's binary_logloss: 0.102967\n",
      "[24]\ttraining's auc: 0.998335\ttraining's binary_logloss: 0.0506578\tvalid_1's auc: 0.845712\tvalid_1's binary_logloss: 0.102734\n",
      "[25]\ttraining's auc: 0.998833\ttraining's binary_logloss: 0.0491006\tvalid_1's auc: 0.851345\tvalid_1's binary_logloss: 0.102186\n",
      "[26]\ttraining's auc: 0.999296\ttraining's binary_logloss: 0.0475893\tvalid_1's auc: 0.853121\tvalid_1's binary_logloss: 0.101716\n",
      "[27]\ttraining's auc: 0.999567\ttraining's binary_logloss: 0.0462838\tvalid_1's auc: 0.85545\tvalid_1's binary_logloss: 0.101652\n",
      "[28]\ttraining's auc: 0.999803\ttraining's binary_logloss: 0.0447966\tvalid_1's auc: 0.857342\tvalid_1's binary_logloss: 0.101496\n",
      "[29]\ttraining's auc: 0.999882\ttraining's binary_logloss: 0.0435535\tvalid_1's auc: 0.858332\tvalid_1's binary_logloss: 0.101553\n",
      "[30]\ttraining's auc: 0.999914\ttraining's binary_logloss: 0.0423732\tvalid_1's auc: 0.859511\tvalid_1's binary_logloss: 0.101475\n",
      "[31]\ttraining's auc: 0.999947\ttraining's binary_logloss: 0.0411068\tvalid_1's auc: 0.861942\tvalid_1's binary_logloss: 0.101292\n",
      "[32]\ttraining's auc: 0.999971\ttraining's binary_logloss: 0.0398649\tvalid_1's auc: 0.86366\tvalid_1's binary_logloss: 0.100975\n",
      "[33]\ttraining's auc: 0.999983\ttraining's binary_logloss: 0.0388913\tvalid_1's auc: 0.865989\tvalid_1's binary_logloss: 0.100898\n",
      "[34]\ttraining's auc: 0.999989\ttraining's binary_logloss: 0.0378302\tvalid_1's auc: 0.864563\tvalid_1's binary_logloss: 0.101349\n",
      "[35]\ttraining's auc: 0.999992\ttraining's binary_logloss: 0.0367609\tvalid_1's auc: 0.862234\tvalid_1's binary_logloss: 0.102093\n",
      "[36]\ttraining's auc: 0.999996\ttraining's binary_logloss: 0.0358175\tvalid_1's auc: 0.864854\tvalid_1's binary_logloss: 0.101397\n",
      "[37]\ttraining's auc: 0.999997\ttraining's binary_logloss: 0.0348532\tvalid_1's auc: 0.868202\tvalid_1's binary_logloss: 0.101137\n",
      "[38]\ttraining's auc: 0.999999\ttraining's binary_logloss: 0.033895\tvalid_1's auc: 0.866688\tvalid_1's binary_logloss: 0.101482\n",
      "[39]\ttraining's auc: 1\ttraining's binary_logloss: 0.0330077\tvalid_1's auc: 0.866135\tvalid_1's binary_logloss: 0.101685\n",
      "[40]\ttraining's auc: 1\ttraining's binary_logloss: 0.0320025\tvalid_1's auc: 0.867329\tvalid_1's binary_logloss: 0.101218\n",
      "[41]\ttraining's auc: 1\ttraining's binary_logloss: 0.0311221\tvalid_1's auc: 0.867678\tvalid_1's binary_logloss: 0.100999\n",
      "[42]\ttraining's auc: 1\ttraining's binary_logloss: 0.030307\tvalid_1's auc: 0.867678\tvalid_1's binary_logloss: 0.101169\n",
      "[43]\ttraining's auc: 1\ttraining's binary_logloss: 0.0294473\tvalid_1's auc: 0.86762\tvalid_1's binary_logloss: 0.101359\n",
      "Early stopping, best iteration is:\n",
      "[33]\ttraining's auc: 0.999983\ttraining's binary_logloss: 0.0388913\tvalid_1's auc: 0.865989\tvalid_1's binary_logloss: 0.100898\n",
      "[1]\ttraining's auc: 0.837916\ttraining's binary_logloss: 0.132621\tvalid_1's auc: 0.724045\tvalid_1's binary_logloss: 0.117544\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's auc: 0.86754\ttraining's binary_logloss: 0.122759\tvalid_1's auc: 0.763063\tvalid_1's binary_logloss: 0.115845\n",
      "[3]\ttraining's auc: 0.893531\ttraining's binary_logloss: 0.114724\tvalid_1's auc: 0.756462\tvalid_1's binary_logloss: 0.115374\n",
      "[4]\ttraining's auc: 0.914927\ttraining's binary_logloss: 0.108198\tvalid_1's auc: 0.755219\tvalid_1's binary_logloss: 0.114569\n",
      "[5]\ttraining's auc: 0.918369\ttraining's binary_logloss: 0.102803\tvalid_1's auc: 0.753976\tvalid_1's binary_logloss: 0.113389\n",
      "[6]\ttraining's auc: 0.932832\ttraining's binary_logloss: 0.0976461\tvalid_1's auc: 0.772181\tvalid_1's binary_logloss: 0.112674\n",
      "[7]\ttraining's auc: 0.938852\ttraining's binary_logloss: 0.0933507\tvalid_1's auc: 0.779776\tvalid_1's binary_logloss: 0.111619\n",
      "[8]\ttraining's auc: 0.943999\ttraining's binary_logloss: 0.0893729\tvalid_1's auc: 0.780848\tvalid_1's binary_logloss: 0.111243\n",
      "[9]\ttraining's auc: 0.952349\ttraining's binary_logloss: 0.0855847\tvalid_1's auc: 0.768577\tvalid_1's binary_logloss: 0.111468\n",
      "[10]\ttraining's auc: 0.956088\ttraining's binary_logloss: 0.0819114\tvalid_1's auc: 0.769323\tvalid_1's binary_logloss: 0.111069\n",
      "[11]\ttraining's auc: 0.962611\ttraining's binary_logloss: 0.078531\tvalid_1's auc: 0.76488\tvalid_1's binary_logloss: 0.111152\n",
      "[12]\ttraining's auc: 0.96764\ttraining's binary_logloss: 0.0755713\tvalid_1's auc: 0.761246\tvalid_1's binary_logloss: 0.110355\n",
      "[13]\ttraining's auc: 0.975022\ttraining's binary_logloss: 0.0726854\tvalid_1's auc: 0.754473\tvalid_1's binary_logloss: 0.110633\n",
      "[14]\ttraining's auc: 0.980237\ttraining's binary_logloss: 0.0696426\tvalid_1's auc: 0.763482\tvalid_1's binary_logloss: 0.110436\n",
      "[15]\ttraining's auc: 0.984318\ttraining's binary_logloss: 0.0669793\tvalid_1's auc: 0.764181\tvalid_1's binary_logloss: 0.110104\n",
      "[16]\ttraining's auc: 0.986156\ttraining's binary_logloss: 0.0647646\tvalid_1's auc: 0.770006\tvalid_1's binary_logloss: 0.109917\n",
      "[17]\ttraining's auc: 0.987133\ttraining's binary_logloss: 0.062547\tvalid_1's auc: 0.767055\tvalid_1's binary_logloss: 0.110528\n",
      "[18]\ttraining's auc: 0.989838\ttraining's binary_logloss: 0.0604197\tvalid_1's auc: 0.780584\tvalid_1's binary_logloss: 0.110213\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's auc: 0.943999\ttraining's binary_logloss: 0.0893729\tvalid_1's auc: 0.780848\tvalid_1's binary_logloss: 0.111243\n",
      "[1]\ttraining's auc: 0.857887\ttraining's binary_logloss: 0.131181\tvalid_1's auc: 0.690055\tvalid_1's binary_logloss: 0.125533\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's auc: 0.87847\ttraining's binary_logloss: 0.120572\tvalid_1's auc: 0.702885\tvalid_1's binary_logloss: 0.123831\n",
      "[3]\ttraining's auc: 0.889859\ttraining's binary_logloss: 0.112861\tvalid_1's auc: 0.710733\tvalid_1's binary_logloss: 0.122362\n",
      "[4]\ttraining's auc: 0.898494\ttraining's binary_logloss: 0.106406\tvalid_1's auc: 0.718355\tvalid_1's binary_logloss: 0.121436\n",
      "[5]\ttraining's auc: 0.913828\ttraining's binary_logloss: 0.100895\tvalid_1's auc: 0.738595\tvalid_1's binary_logloss: 0.120731\n",
      "[6]\ttraining's auc: 0.929534\ttraining's binary_logloss: 0.0956002\tvalid_1's auc: 0.72904\tvalid_1's binary_logloss: 0.120563\n",
      "[7]\ttraining's auc: 0.932201\ttraining's binary_logloss: 0.0916861\tvalid_1's auc: 0.72791\tvalid_1's binary_logloss: 0.120436\n",
      "[8]\ttraining's auc: 0.937436\ttraining's binary_logloss: 0.0879966\tvalid_1's auc: 0.753543\tvalid_1's binary_logloss: 0.11973\n",
      "[9]\ttraining's auc: 0.947842\ttraining's binary_logloss: 0.0842907\tvalid_1's auc: 0.744255\tvalid_1's binary_logloss: 0.119443\n",
      "[10]\ttraining's auc: 0.954202\ttraining's binary_logloss: 0.0811412\tvalid_1's auc: 0.751313\tvalid_1's binary_logloss: 0.118713\n",
      "[11]\ttraining's auc: 0.965417\ttraining's binary_logloss: 0.0775952\tvalid_1's auc: 0.764708\tvalid_1's binary_logloss: 0.118401\n",
      "[12]\ttraining's auc: 0.974365\ttraining's binary_logloss: 0.0748251\tvalid_1's auc: 0.757509\tvalid_1's binary_logloss: 0.118721\n",
      "[13]\ttraining's auc: 0.977261\ttraining's binary_logloss: 0.0721171\tvalid_1's auc: 0.758568\tvalid_1's binary_logloss: 0.118652\n",
      "[14]\ttraining's auc: 0.978819\ttraining's binary_logloss: 0.0695777\tvalid_1's auc: 0.766444\tvalid_1's binary_logloss: 0.118057\n",
      "[15]\ttraining's auc: 0.984326\ttraining's binary_logloss: 0.0671296\tvalid_1's auc: 0.768871\tvalid_1's binary_logloss: 0.117986\n",
      "[16]\ttraining's auc: 0.986539\ttraining's binary_logloss: 0.0649958\tvalid_1's auc: 0.775491\tvalid_1's binary_logloss: 0.118115\n",
      "[17]\ttraining's auc: 0.987724\ttraining's binary_logloss: 0.0629554\tvalid_1's auc: 0.775265\tvalid_1's binary_logloss: 0.117874\n",
      "[18]\ttraining's auc: 0.989766\ttraining's binary_logloss: 0.0609493\tvalid_1's auc: 0.774771\tvalid_1's binary_logloss: 0.117782\n",
      "[19]\ttraining's auc: 0.991589\ttraining's binary_logloss: 0.0588383\tvalid_1's auc: 0.784539\tvalid_1's binary_logloss: 0.117799\n",
      "[20]\ttraining's auc: 0.994975\ttraining's binary_logloss: 0.0566084\tvalid_1's auc: 0.780925\tvalid_1's binary_logloss: 0.117852\n",
      "[21]\ttraining's auc: 0.996006\ttraining's binary_logloss: 0.0548414\tvalid_1's auc: 0.777975\tvalid_1's binary_logloss: 0.118244\n",
      "[22]\ttraining's auc: 0.997746\ttraining's binary_logloss: 0.0529957\tvalid_1's auc: 0.781561\tvalid_1's binary_logloss: 0.118103\n",
      "[23]\ttraining's auc: 0.998706\ttraining's binary_logloss: 0.0512984\tvalid_1's auc: 0.778766\tvalid_1's binary_logloss: 0.118189\n",
      "[24]\ttraining's auc: 0.998798\ttraining's binary_logloss: 0.0499318\tvalid_1's auc: 0.77806\tvalid_1's binary_logloss: 0.118428\n",
      "[25]\ttraining's auc: 0.99913\ttraining's binary_logloss: 0.0485603\tvalid_1's auc: 0.778455\tvalid_1's binary_logloss: 0.118453\n",
      "[26]\ttraining's auc: 0.999456\ttraining's binary_logloss: 0.0471351\tvalid_1's auc: 0.777552\tvalid_1's binary_logloss: 0.118713\n",
      "[27]\ttraining's auc: 0.999691\ttraining's binary_logloss: 0.0455307\tvalid_1's auc: 0.771497\tvalid_1's binary_logloss: 0.119382\n",
      "[28]\ttraining's auc: 0.999788\ttraining's binary_logloss: 0.0441034\tvalid_1's auc: 0.772908\tvalid_1's binary_logloss: 0.119335\n",
      "Early stopping, best iteration is:\n",
      "[18]\ttraining's auc: 0.989766\ttraining's binary_logloss: 0.0609493\tvalid_1's auc: 0.774771\tvalid_1's binary_logloss: 0.117782\n",
      "[1]\ttraining's auc: 0.837407\ttraining's binary_logloss: 0.129603\tvalid_1's auc: 0.821131\tvalid_1's binary_logloss: 0.142665\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's auc: 0.866286\ttraining's binary_logloss: 0.119993\tvalid_1's auc: 0.832541\tvalid_1's binary_logloss: 0.138584\n",
      "[3]\ttraining's auc: 0.881594\ttraining's binary_logloss: 0.112476\tvalid_1's auc: 0.841399\tvalid_1's binary_logloss: 0.13645\n",
      "[4]\ttraining's auc: 0.89878\ttraining's binary_logloss: 0.106244\tvalid_1's auc: 0.839079\tvalid_1's binary_logloss: 0.135157\n",
      "[5]\ttraining's auc: 0.915335\ttraining's binary_logloss: 0.100657\tvalid_1's auc: 0.855699\tvalid_1's binary_logloss: 0.131845\n",
      "[6]\ttraining's auc: 0.922451\ttraining's binary_logloss: 0.0960827\tvalid_1's auc: 0.863112\tvalid_1's binary_logloss: 0.128818\n",
      "[7]\ttraining's auc: 0.930837\ttraining's binary_logloss: 0.0919862\tvalid_1's auc: 0.861131\tvalid_1's binary_logloss: 0.127159\n",
      "[8]\ttraining's auc: 0.935388\ttraining's binary_logloss: 0.0880981\tvalid_1's auc: 0.872517\tvalid_1's binary_logloss: 0.124693\n",
      "[9]\ttraining's auc: 0.950838\ttraining's binary_logloss: 0.0847195\tvalid_1's auc: 0.87021\tvalid_1's binary_logloss: 0.124437\n",
      "[10]\ttraining's auc: 0.956847\ttraining's binary_logloss: 0.0811628\tvalid_1's auc: 0.870734\tvalid_1's binary_logloss: 0.123502\n",
      "[11]\ttraining's auc: 0.959233\ttraining's binary_logloss: 0.0781924\tvalid_1's auc: 0.876434\tvalid_1's binary_logloss: 0.121723\n",
      "[12]\ttraining's auc: 0.975104\ttraining's binary_logloss: 0.0750985\tvalid_1's auc: 0.869114\tvalid_1's binary_logloss: 0.121706\n",
      "[13]\ttraining's auc: 0.977152\ttraining's binary_logloss: 0.0722125\tvalid_1's auc: 0.87303\tvalid_1's binary_logloss: 0.120281\n",
      "[14]\ttraining's auc: 0.979529\ttraining's binary_logloss: 0.0695113\tvalid_1's auc: 0.872587\tvalid_1's binary_logloss: 0.120257\n",
      "[15]\ttraining's auc: 0.983679\ttraining's binary_logloss: 0.0670713\tvalid_1's auc: 0.867541\tvalid_1's binary_logloss: 0.120576\n",
      "[16]\ttraining's auc: 0.984896\ttraining's binary_logloss: 0.0649644\tvalid_1's auc: 0.867914\tvalid_1's binary_logloss: 0.12051\n",
      "[17]\ttraining's auc: 0.98853\ttraining's binary_logloss: 0.0625296\tvalid_1's auc: 0.868718\tvalid_1's binary_logloss: 0.120304\n",
      "[18]\ttraining's auc: 0.992311\ttraining's binary_logloss: 0.0602941\tvalid_1's auc: 0.8788\tvalid_1's binary_logloss: 0.120209\n",
      "[19]\ttraining's auc: 0.993145\ttraining's binary_logloss: 0.0582181\tvalid_1's auc: 0.880221\tvalid_1's binary_logloss: 0.119984\n",
      "[20]\ttraining's auc: 0.996245\ttraining's binary_logloss: 0.0561041\tvalid_1's auc: 0.879476\tvalid_1's binary_logloss: 0.119922\n",
      "[21]\ttraining's auc: 0.997961\ttraining's binary_logloss: 0.0540036\tvalid_1's auc: 0.878753\tvalid_1's binary_logloss: 0.119733\n",
      "[22]\ttraining's auc: 0.998721\ttraining's binary_logloss: 0.0521994\tvalid_1's auc: 0.876515\tvalid_1's binary_logloss: 0.119884\n",
      "[23]\ttraining's auc: 0.999296\ttraining's binary_logloss: 0.0505914\tvalid_1's auc: 0.874289\tvalid_1's binary_logloss: 0.120077\n",
      "[24]\ttraining's auc: 0.999472\ttraining's binary_logloss: 0.0489803\tvalid_1's auc: 0.87359\tvalid_1's binary_logloss: 0.120408\n",
      "[25]\ttraining's auc: 0.999656\ttraining's binary_logloss: 0.0475175\tvalid_1's auc: 0.87359\tvalid_1's binary_logloss: 0.120127\n",
      "[26]\ttraining's auc: 0.999813\ttraining's binary_logloss: 0.0460764\tvalid_1's auc: 0.873124\tvalid_1's binary_logloss: 0.120227\n",
      "[27]\ttraining's auc: 0.999864\ttraining's binary_logloss: 0.0446193\tvalid_1's auc: 0.875175\tvalid_1's binary_logloss: 0.120005\n",
      "[28]\ttraining's auc: 0.999911\ttraining's binary_logloss: 0.0432511\tvalid_1's auc: 0.872727\tvalid_1's binary_logloss: 0.120442\n",
      "[29]\ttraining's auc: 0.999945\ttraining's binary_logloss: 0.042012\tvalid_1's auc: 0.872448\tvalid_1's binary_logloss: 0.120734\n",
      "Early stopping, best iteration is:\n",
      "[19]\ttraining's auc: 0.993145\ttraining's binary_logloss: 0.0582181\tvalid_1's auc: 0.880221\tvalid_1's binary_logloss: 0.119984\n",
      "[1]\ttraining's auc: 0.839952\ttraining's binary_logloss: 0.128382\tvalid_1's auc: 0.7453\tvalid_1's binary_logloss: 0.152936\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's auc: 0.885917\ttraining's binary_logloss: 0.118606\tvalid_1's auc: 0.76463\tvalid_1's binary_logloss: 0.151042\n",
      "[3]\ttraining's auc: 0.902828\ttraining's binary_logloss: 0.110918\tvalid_1's auc: 0.76723\tvalid_1's binary_logloss: 0.148944\n",
      "[4]\ttraining's auc: 0.911838\ttraining's binary_logloss: 0.104792\tvalid_1's auc: 0.761872\tvalid_1's binary_logloss: 0.146671\n",
      "[5]\ttraining's auc: 0.916614\ttraining's binary_logloss: 0.0995594\tvalid_1's auc: 0.770115\tvalid_1's binary_logloss: 0.145154\n",
      "[6]\ttraining's auc: 0.931445\ttraining's binary_logloss: 0.0946401\tvalid_1's auc: 0.77663\tvalid_1's binary_logloss: 0.143828\n",
      "[7]\ttraining's auc: 0.933632\ttraining's binary_logloss: 0.0905345\tvalid_1's auc: 0.785085\tvalid_1's binary_logloss: 0.141658\n",
      "[8]\ttraining's auc: 0.937655\ttraining's binary_logloss: 0.0867065\tvalid_1's auc: 0.802761\tvalid_1's binary_logloss: 0.139677\n",
      "[9]\ttraining's auc: 0.94815\ttraining's binary_logloss: 0.0830965\tvalid_1's auc: 0.817635\tvalid_1's binary_logloss: 0.137629\n",
      "[10]\ttraining's auc: 0.954859\ttraining's binary_logloss: 0.0795735\tvalid_1's auc: 0.817189\tvalid_1's binary_logloss: 0.137324\n",
      "[11]\ttraining's auc: 0.95891\ttraining's binary_logloss: 0.0766033\tvalid_1's auc: 0.817879\tvalid_1's binary_logloss: 0.136816\n",
      "[12]\ttraining's auc: 0.964821\ttraining's binary_logloss: 0.0738027\tvalid_1's auc: 0.819704\tvalid_1's binary_logloss: 0.135915\n",
      "[13]\ttraining's auc: 0.96697\ttraining's binary_logloss: 0.0711011\tvalid_1's auc: 0.818431\tvalid_1's binary_logloss: 0.135951\n",
      "[14]\ttraining's auc: 0.974458\ttraining's binary_logloss: 0.0685433\tvalid_1's auc: 0.821656\tvalid_1's binary_logloss: 0.135938\n",
      "[15]\ttraining's auc: 0.97746\ttraining's binary_logloss: 0.066303\tvalid_1's auc: 0.816341\tvalid_1's binary_logloss: 0.135773\n",
      "[16]\ttraining's auc: 0.978001\ttraining's binary_logloss: 0.0641465\tvalid_1's auc: 0.819386\tvalid_1's binary_logloss: 0.135325\n",
      "[17]\ttraining's auc: 0.980259\ttraining's binary_logloss: 0.0618773\tvalid_1's auc: 0.814898\tvalid_1's binary_logloss: 0.135178\n",
      "[18]\ttraining's auc: 0.982182\ttraining's binary_logloss: 0.059702\tvalid_1's auc: 0.816044\tvalid_1's binary_logloss: 0.135274\n",
      "[19]\ttraining's auc: 0.986879\ttraining's binary_logloss: 0.0576109\tvalid_1's auc: 0.811322\tvalid_1's binary_logloss: 0.135204\n",
      "[20]\ttraining's auc: 0.99135\ttraining's binary_logloss: 0.0557739\tvalid_1's auc: 0.808511\tvalid_1's binary_logloss: 0.135137\n",
      "[21]\ttraining's auc: 0.99511\ttraining's binary_logloss: 0.0536169\tvalid_1's auc: 0.806209\tvalid_1's binary_logloss: 0.135881\n",
      "[22]\ttraining's auc: 0.996262\ttraining's binary_logloss: 0.0519396\tvalid_1's auc: 0.816648\tvalid_1's binary_logloss: 0.135403\n",
      "[23]\ttraining's auc: 0.99824\ttraining's binary_logloss: 0.0501747\tvalid_1's auc: 0.821911\tvalid_1's binary_logloss: 0.135177\n",
      "[24]\ttraining's auc: 0.998309\ttraining's binary_logloss: 0.0485815\tvalid_1's auc: 0.823502\tvalid_1's binary_logloss: 0.13499\n",
      "[25]\ttraining's auc: 0.999108\ttraining's binary_logloss: 0.0469517\tvalid_1's auc: 0.821507\tvalid_1's binary_logloss: 0.135551\n",
      "[26]\ttraining's auc: 0.99943\ttraining's binary_logloss: 0.0455048\tvalid_1's auc: 0.821295\tvalid_1's binary_logloss: 0.135754\n",
      "[27]\ttraining's auc: 0.999676\ttraining's binary_logloss: 0.044119\tvalid_1's auc: 0.823035\tvalid_1's binary_logloss: 0.135801\n",
      "[28]\ttraining's auc: 0.999823\ttraining's binary_logloss: 0.0428368\tvalid_1's auc: 0.826006\tvalid_1's binary_logloss: 0.135156\n",
      "[29]\ttraining's auc: 0.999914\ttraining's binary_logloss: 0.0416493\tvalid_1's auc: 0.825242\tvalid_1's binary_logloss: 0.135238\n",
      "[30]\ttraining's auc: 0.999947\ttraining's binary_logloss: 0.0404854\tvalid_1's auc: 0.82539\tvalid_1's binary_logloss: 0.135357\n",
      "[31]\ttraining's auc: 0.999965\ttraining's binary_logloss: 0.0392696\tvalid_1's auc: 0.826982\tvalid_1's binary_logloss: 0.135402\n",
      "[32]\ttraining's auc: 0.999981\ttraining's binary_logloss: 0.0381775\tvalid_1's auc: 0.828807\tvalid_1's binary_logloss: 0.134975\n",
      "[33]\ttraining's auc: 0.99999\ttraining's binary_logloss: 0.0370068\tvalid_1's auc: 0.82974\tvalid_1's binary_logloss: 0.13559\n",
      "[34]\ttraining's auc: 0.999994\ttraining's binary_logloss: 0.0359051\tvalid_1's auc: 0.830207\tvalid_1's binary_logloss: 0.13617\n",
      "[35]\ttraining's auc: 0.999997\ttraining's binary_logloss: 0.0349594\tvalid_1's auc: 0.828743\tvalid_1's binary_logloss: 0.136726\n",
      "[36]\ttraining's auc: 0.999999\ttraining's binary_logloss: 0.0340027\tvalid_1's auc: 0.826282\tvalid_1's binary_logloss: 0.137125\n",
      "[37]\ttraining's auc: 0.999999\ttraining's binary_logloss: 0.0331022\tvalid_1's auc: 0.828446\tvalid_1's binary_logloss: 0.137177\n",
      "[38]\ttraining's auc: 0.999999\ttraining's binary_logloss: 0.0321478\tvalid_1's auc: 0.828149\tvalid_1's binary_logloss: 0.137263\n",
      "[39]\ttraining's auc: 1\ttraining's binary_logloss: 0.0312359\tvalid_1's auc: 0.83078\tvalid_1's binary_logloss: 0.137579\n",
      "[40]\ttraining's auc: 1\ttraining's binary_logloss: 0.0304318\tvalid_1's auc: 0.832244\tvalid_1's binary_logloss: 0.137833\n",
      "[41]\ttraining's auc: 1\ttraining's binary_logloss: 0.0296105\tvalid_1's auc: 0.830992\tvalid_1's binary_logloss: 0.138394\n",
      "[42]\ttraining's auc: 1\ttraining's binary_logloss: 0.0287445\tvalid_1's auc: 0.83165\tvalid_1's binary_logloss: 0.138908\n",
      "Early stopping, best iteration is:\n",
      "[32]\ttraining's auc: 0.999981\ttraining's binary_logloss: 0.0381775\tvalid_1's auc: 0.828807\tvalid_1's binary_logloss: 0.134975\n",
      "[1]\ttraining's auc: 0.849503\ttraining's binary_logloss: 0.127546\tvalid_1's auc: 0.742156\tvalid_1's binary_logloss: 0.163462\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's auc: 0.881572\ttraining's binary_logloss: 0.117632\tvalid_1's auc: 0.8061\tvalid_1's binary_logloss: 0.159329\n",
      "[3]\ttraining's auc: 0.902643\ttraining's binary_logloss: 0.109846\tvalid_1's auc: 0.809932\tvalid_1's binary_logloss: 0.157855\n",
      "[4]\ttraining's auc: 0.913708\ttraining's binary_logloss: 0.104083\tvalid_1's auc: 0.817646\tvalid_1's binary_logloss: 0.154341\n",
      "[5]\ttraining's auc: 0.926075\ttraining's binary_logloss: 0.0987831\tvalid_1's auc: 0.829222\tvalid_1's binary_logloss: 0.153636\n",
      "[6]\ttraining's auc: 0.933462\ttraining's binary_logloss: 0.0939913\tvalid_1's auc: 0.817357\tvalid_1's binary_logloss: 0.153508\n",
      "[7]\ttraining's auc: 0.93595\ttraining's binary_logloss: 0.0896174\tvalid_1's auc: 0.816999\tvalid_1's binary_logloss: 0.152849\n",
      "[8]\ttraining's auc: 0.940583\ttraining's binary_logloss: 0.0858521\tvalid_1's auc: 0.820752\tvalid_1's binary_logloss: 0.151374\n",
      "[9]\ttraining's auc: 0.945667\ttraining's binary_logloss: 0.0824449\tvalid_1's auc: 0.821727\tvalid_1's binary_logloss: 0.15054\n",
      "[10]\ttraining's auc: 0.950185\ttraining's binary_logloss: 0.0789373\tvalid_1's auc: 0.827938\tvalid_1's binary_logloss: 0.148539\n",
      "[11]\ttraining's auc: 0.9559\ttraining's binary_logloss: 0.0760118\tvalid_1's auc: 0.824693\tvalid_1's binary_logloss: 0.148778\n",
      "[12]\ttraining's auc: 0.966305\ttraining's binary_logloss: 0.0728316\tvalid_1's auc: 0.828675\tvalid_1's binary_logloss: 0.147448\n",
      "[13]\ttraining's auc: 0.96826\ttraining's binary_logloss: 0.0705123\tvalid_1's auc: 0.830417\tvalid_1's binary_logloss: 0.146987\n",
      "[14]\ttraining's auc: 0.972689\ttraining's binary_logloss: 0.0679217\tvalid_1's auc: 0.82749\tvalid_1's binary_logloss: 0.147666\n",
      "[15]\ttraining's auc: 0.975743\ttraining's binary_logloss: 0.0656991\tvalid_1's auc: 0.822086\tvalid_1's binary_logloss: 0.147707\n",
      "[16]\ttraining's auc: 0.982367\ttraining's binary_logloss: 0.0632052\tvalid_1's auc: 0.820174\tvalid_1's binary_logloss: 0.147609\n",
      "[17]\ttraining's auc: 0.987085\ttraining's binary_logloss: 0.0608035\tvalid_1's auc: 0.818184\tvalid_1's binary_logloss: 0.147072\n",
      "[18]\ttraining's auc: 0.989308\ttraining's binary_logloss: 0.0585979\tvalid_1's auc: 0.824514\tvalid_1's binary_logloss: 0.146822\n",
      "[19]\ttraining's auc: 0.993145\ttraining's binary_logloss: 0.0563816\tvalid_1's auc: 0.826216\tvalid_1's binary_logloss: 0.146505\n",
      "[20]\ttraining's auc: 0.995589\ttraining's binary_logloss: 0.0542832\tvalid_1's auc: 0.824683\tvalid_1's binary_logloss: 0.146604\n",
      "[21]\ttraining's auc: 0.996727\ttraining's binary_logloss: 0.0525256\tvalid_1's auc: 0.830835\tvalid_1's binary_logloss: 0.146366\n",
      "[22]\ttraining's auc: 0.99742\ttraining's binary_logloss: 0.0508569\tvalid_1's auc: 0.834876\tvalid_1's binary_logloss: 0.145576\n",
      "[23]\ttraining's auc: 0.998158\ttraining's binary_logloss: 0.0493207\tvalid_1's auc: 0.834458\tvalid_1's binary_logloss: 0.145318\n",
      "[24]\ttraining's auc: 0.99887\ttraining's binary_logloss: 0.0478756\tvalid_1's auc: 0.835663\tvalid_1's binary_logloss: 0.144918\n",
      "[25]\ttraining's auc: 0.999136\ttraining's binary_logloss: 0.0464199\tvalid_1's auc: 0.832935\tvalid_1's binary_logloss: 0.145488\n",
      "[26]\ttraining's auc: 0.999434\ttraining's binary_logloss: 0.0450573\tvalid_1's auc: 0.834637\tvalid_1's binary_logloss: 0.145462\n",
      "[27]\ttraining's auc: 0.999604\ttraining's binary_logloss: 0.0436411\tvalid_1's auc: 0.835025\tvalid_1's binary_logloss: 0.145672\n",
      "[28]\ttraining's auc: 0.999762\ttraining's binary_logloss: 0.0423319\tvalid_1's auc: 0.837235\tvalid_1's binary_logloss: 0.14562\n",
      "[29]\ttraining's auc: 0.999847\ttraining's binary_logloss: 0.0411282\tvalid_1's auc: 0.83624\tvalid_1's binary_logloss: 0.1459\n",
      "[30]\ttraining's auc: 0.999903\ttraining's binary_logloss: 0.0399528\tvalid_1's auc: 0.836449\tvalid_1's binary_logloss: 0.145733\n",
      "[31]\ttraining's auc: 0.99992\ttraining's binary_logloss: 0.0387846\tvalid_1's auc: 0.835195\tvalid_1's binary_logloss: 0.146731\n",
      "[32]\ttraining's auc: 0.999933\ttraining's binary_logloss: 0.0377738\tvalid_1's auc: 0.834578\tvalid_1's binary_logloss: 0.147466\n",
      "[33]\ttraining's auc: 0.999957\ttraining's binary_logloss: 0.0365554\tvalid_1's auc: 0.833423\tvalid_1's binary_logloss: 0.147962\n",
      "[34]\ttraining's auc: 0.999974\ttraining's binary_logloss: 0.0354596\tvalid_1's auc: 0.832517\tvalid_1's binary_logloss: 0.148514\n",
      "Early stopping, best iteration is:\n",
      "[24]\ttraining's auc: 0.99887\ttraining's binary_logloss: 0.0478756\tvalid_1's auc: 0.835663\tvalid_1's binary_logloss: 0.144918\n",
      "[1]\ttraining's auc: 0.837629\ttraining's binary_logloss: 0.130772\tvalid_1's auc: 0.678672\tvalid_1's binary_logloss: 0.139126\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's auc: 0.863155\ttraining's binary_logloss: 0.120085\tvalid_1's auc: 0.714498\tvalid_1's binary_logloss: 0.136825\n",
      "[3]\ttraining's auc: 0.869917\ttraining's binary_logloss: 0.112837\tvalid_1's auc: 0.718445\tvalid_1's binary_logloss: 0.134879\n",
      "[4]\ttraining's auc: 0.895573\ttraining's binary_logloss: 0.10655\tvalid_1's auc: 0.791902\tvalid_1's binary_logloss: 0.132457\n",
      "[5]\ttraining's auc: 0.904885\ttraining's binary_logloss: 0.101326\tvalid_1's auc: 0.798876\tvalid_1's binary_logloss: 0.131446\n",
      "[6]\ttraining's auc: 0.91303\ttraining's binary_logloss: 0.0961766\tvalid_1's auc: 0.813349\tvalid_1's binary_logloss: 0.128904\n",
      "[7]\ttraining's auc: 0.919647\ttraining's binary_logloss: 0.0921004\tvalid_1's auc: 0.825861\tvalid_1's binary_logloss: 0.128325\n",
      "[8]\ttraining's auc: 0.929862\ttraining's binary_logloss: 0.0882447\tvalid_1's auc: 0.838421\tvalid_1's binary_logloss: 0.127531\n",
      "[9]\ttraining's auc: 0.940541\ttraining's binary_logloss: 0.0846662\tvalid_1's auc: 0.842739\tvalid_1's binary_logloss: 0.126517\n",
      "[10]\ttraining's auc: 0.949071\ttraining's binary_logloss: 0.0811614\tvalid_1's auc: 0.840754\tvalid_1's binary_logloss: 0.126678\n",
      "[11]\ttraining's auc: 0.954955\ttraining's binary_logloss: 0.0781736\tvalid_1's auc: 0.837691\tvalid_1's binary_logloss: 0.126187\n",
      "[12]\ttraining's auc: 0.957002\ttraining's binary_logloss: 0.075348\tvalid_1's auc: 0.832165\tvalid_1's binary_logloss: 0.126508\n",
      "[13]\ttraining's auc: 0.963673\ttraining's binary_logloss: 0.0726712\tvalid_1's auc: 0.830742\tvalid_1's binary_logloss: 0.12638\n",
      "[14]\ttraining's auc: 0.969925\ttraining's binary_logloss: 0.0699044\tvalid_1's auc: 0.826435\tvalid_1's binary_logloss: 0.1265\n",
      "[15]\ttraining's auc: 0.97699\ttraining's binary_logloss: 0.0673481\tvalid_1's auc: 0.827344\tvalid_1's binary_logloss: 0.125952\n",
      "[16]\ttraining's auc: 0.985143\ttraining's binary_logloss: 0.0646696\tvalid_1's auc: 0.823612\tvalid_1's binary_logloss: 0.125678\n",
      "[17]\ttraining's auc: 0.985878\ttraining's binary_logloss: 0.0626144\tvalid_1's auc: 0.820215\tvalid_1's binary_logloss: 0.125818\n",
      "[18]\ttraining's auc: 0.988351\ttraining's binary_logloss: 0.0604112\tvalid_1's auc: 0.82384\tvalid_1's binary_logloss: 0.125421\n",
      "[19]\ttraining's auc: 0.992918\ttraining's binary_logloss: 0.0584082\tvalid_1's auc: 0.822297\tvalid_1's binary_logloss: 0.125307\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttraining's auc: 0.940541\ttraining's binary_logloss: 0.0846662\tvalid_1's auc: 0.842739\tvalid_1's binary_logloss: 0.126517\n",
      "[1]\ttraining's auc: 0.807183\ttraining's binary_logloss: 0.127209\tvalid_1's auc: 0.738543\tvalid_1's binary_logloss: 0.159881\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's auc: 0.871934\ttraining's binary_logloss: 0.117166\tvalid_1's auc: 0.808518\tvalid_1's binary_logloss: 0.156343\n",
      "[3]\ttraining's auc: 0.887481\ttraining's binary_logloss: 0.109635\tvalid_1's auc: 0.809185\tvalid_1's binary_logloss: 0.155848\n",
      "[4]\ttraining's auc: 0.909886\ttraining's binary_logloss: 0.103506\tvalid_1's auc: 0.819617\tvalid_1's binary_logloss: 0.153227\n",
      "[5]\ttraining's auc: 0.913633\ttraining's binary_logloss: 0.0984025\tvalid_1's auc: 0.82337\tvalid_1's binary_logloss: 0.151415\n",
      "[6]\ttraining's auc: 0.923628\ttraining's binary_logloss: 0.0935258\tvalid_1's auc: 0.837793\tvalid_1's binary_logloss: 0.149175\n",
      "[7]\ttraining's auc: 0.934368\ttraining's binary_logloss: 0.0893316\tvalid_1's auc: 0.835424\tvalid_1's binary_logloss: 0.148332\n",
      "[8]\ttraining's auc: 0.9365\ttraining's binary_logloss: 0.0856607\tvalid_1's auc: 0.832009\tvalid_1's binary_logloss: 0.14813\n",
      "[9]\ttraining's auc: 0.946963\ttraining's binary_logloss: 0.0821449\tvalid_1's auc: 0.830526\tvalid_1's binary_logloss: 0.147674\n",
      "[10]\ttraining's auc: 0.952354\ttraining's binary_logloss: 0.0789414\tvalid_1's auc: 0.827242\tvalid_1's binary_logloss: 0.147502\n",
      "[11]\ttraining's auc: 0.955764\ttraining's binary_logloss: 0.0760198\tvalid_1's auc: 0.829262\tvalid_1's binary_logloss: 0.146857\n",
      "[12]\ttraining's auc: 0.959719\ttraining's binary_logloss: 0.0732909\tvalid_1's auc: 0.841127\tvalid_1's binary_logloss: 0.145304\n",
      "[13]\ttraining's auc: 0.960599\ttraining's binary_logloss: 0.0706255\tvalid_1's auc: 0.839873\tvalid_1's binary_logloss: 0.145333\n",
      "[14]\ttraining's auc: 0.970484\ttraining's binary_logloss: 0.0680931\tvalid_1's auc: 0.839335\tvalid_1's binary_logloss: 0.144773\n",
      "[15]\ttraining's auc: 0.97619\ttraining's binary_logloss: 0.0656514\tvalid_1's auc: 0.841018\tvalid_1's binary_logloss: 0.143733\n",
      "[16]\ttraining's auc: 0.984331\ttraining's binary_logloss: 0.0630508\tvalid_1's auc: 0.843008\tvalid_1's binary_logloss: 0.143298\n",
      "[17]\ttraining's auc: 0.986632\ttraining's binary_logloss: 0.0609337\tvalid_1's auc: 0.84054\tvalid_1's binary_logloss: 0.143132\n",
      "[18]\ttraining's auc: 0.992967\ttraining's binary_logloss: 0.0584651\tvalid_1's auc: 0.840132\tvalid_1's binary_logloss: 0.142552\n",
      "[19]\ttraining's auc: 0.994498\ttraining's binary_logloss: 0.0564077\tvalid_1's auc: 0.837783\tvalid_1's binary_logloss: 0.143048\n",
      "[20]\ttraining's auc: 0.995823\ttraining's binary_logloss: 0.0544274\tvalid_1's auc: 0.835971\tvalid_1's binary_logloss: 0.143156\n",
      "[21]\ttraining's auc: 0.996672\ttraining's binary_logloss: 0.0526009\tvalid_1's auc: 0.840271\tvalid_1's binary_logloss: 0.142173\n",
      "[22]\ttraining's auc: 0.998542\ttraining's binary_logloss: 0.050672\tvalid_1's auc: 0.83828\tvalid_1's binary_logloss: 0.142415\n",
      "[23]\ttraining's auc: 0.998739\ttraining's binary_logloss: 0.0491596\tvalid_1's auc: 0.836847\tvalid_1's binary_logloss: 0.1429\n",
      "[24]\ttraining's auc: 0.999373\ttraining's binary_logloss: 0.0475336\tvalid_1's auc: 0.837345\tvalid_1's binary_logloss: 0.142698\n",
      "[25]\ttraining's auc: 0.999694\ttraining's binary_logloss: 0.0459843\tvalid_1's auc: 0.835901\tvalid_1's binary_logloss: 0.142749\n",
      "[26]\ttraining's auc: 0.999846\ttraining's binary_logloss: 0.0445521\tvalid_1's auc: 0.838649\tvalid_1's binary_logloss: 0.142027\n",
      "Early stopping, best iteration is:\n",
      "[16]\ttraining's auc: 0.984331\ttraining's binary_logloss: 0.0630508\tvalid_1's auc: 0.843008\tvalid_1's binary_logloss: 0.143298\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "fi = []\n",
    "test_probs = []\n",
    "i = 0\n",
    "for train_idx, valid_idx in model_selection.KFold(n_splits=10, shuffle=True).split(X_train):\n",
    "    i += 1\n",
    "    Xt = X_train.iloc[train_idx]\n",
    "    yt = y_train.loc[X_train.index].iloc[train_idx]\n",
    "\n",
    "    Xv = X_train.iloc[valid_idx]\n",
    "    yv = y_train.loc[X_train.index].iloc[valid_idx]\n",
    "    \n",
    "#     gridParams = {\n",
    "#     'learning_rate': [.005, .010],\n",
    "#     'num_leaves': [20, 30, 40],\n",
    "#     'boosting_type' : ['gbdt'],\n",
    "#     'objective' : ['binary'],\n",
    "#     'random_state' : [42] # Updated from 'seed'\n",
    "#     #'reg_alpha' : [1,1.2],\n",
    "#     #'reg_lambda' : [1,1.2,1.4],\n",
    "#     }\n",
    "    \n",
    "    \n",
    "    learner = LGBMClassifier(n_estimators=10000)\n",
    "    \n",
    "#     learner = GridSearchCV(lgbm, gridParams, verbose=1, cv=3, n_jobs=-1)\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)])\n",
    "    \n",
    "    test_probs.append(pd.Series(learner.predict_proba(X_test)[:, -1],\n",
    "                                index=X_test.index, name=\"fold_\" + str(i)))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "\n",
    "test_probs = pd.concat(test_probs, axis=1).mean(axis=1)\n",
    "test_probs.index.name=\"USER_ID\"\n",
    "test_probs.name=\"SCORE\"\n",
    "test_probs.to_csv(\"results.csv\", header=True)\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USER_ID\n",
       "0        0.010284\n",
       "1        0.010119\n",
       "2        0.010089\n",
       "3        0.012576\n",
       "4        0.053407\n",
       "5        0.011337\n",
       "6        0.011756\n",
       "7        0.011048\n",
       "8        0.011547\n",
       "9        0.010859\n",
       "10       0.010274\n",
       "11       0.015873\n",
       "12       0.012369\n",
       "13       0.014635\n",
       "14       0.010718\n",
       "15       0.010353\n",
       "16       0.010861\n",
       "17       0.011778\n",
       "18       0.020020\n",
       "19       0.012492\n",
       "20       0.010204\n",
       "21       0.010965\n",
       "22       0.223338\n",
       "23       0.010823\n",
       "24       0.011011\n",
       "25       0.011454\n",
       "26       0.010249\n",
       "27       0.010679\n",
       "28       0.013061\n",
       "29       0.011053\n",
       "           ...   \n",
       "11646    0.032635\n",
       "11647    0.011944\n",
       "11648    0.089475\n",
       "11649    0.029420\n",
       "11650    0.011438\n",
       "11651    0.067200\n",
       "11652    0.011469\n",
       "11653    0.014446\n",
       "11654    0.011618\n",
       "11655    0.015034\n",
       "11656    0.043947\n",
       "11657    0.017494\n",
       "11658    0.010998\n",
       "11659    0.011648\n",
       "11660    0.017862\n",
       "11661    0.010262\n",
       "11662    0.010677\n",
       "11663    0.037653\n",
       "11664    0.010558\n",
       "11665    0.010630\n",
       "11666    0.012633\n",
       "11667    0.010379\n",
       "11668    0.021852\n",
       "11669    0.016615\n",
       "11670    0.010460\n",
       "11671    0.088296\n",
       "11672    0.010234\n",
       "11673    0.010233\n",
       "11674    0.010220\n",
       "11675    0.010131\n",
       "Name: SCORE, Length: 11676, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PAGE_41                0.039094\n",
       "CONTENT_CATEGORY_16    0.020814\n",
       "PAGE_109               0.019298\n",
       "PAGE_153               0.012735\n",
       "PAGE_110               0.012604\n",
       "MONTH_3                0.012080\n",
       "SITE_ID_3              0.010854\n",
       "DAY_15                 0.010839\n",
       "PAGE_1259              0.010693\n",
       "HOUR_9                 0.010586\n",
       "DAY_2                  0.010264\n",
       "DAY_9                  0.009965\n",
       "PAGE_345               0.009730\n",
       "DAY_20                 0.009551\n",
       "HOUR_10                0.009513\n",
       "PAGE_4                 0.009391\n",
       "DAY_6                  0.009355\n",
       "DAY_7                  0.008784\n",
       "DAY_4                  0.008516\n",
       "PAGE_236               0.008465\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi.sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x158832e4ba8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
