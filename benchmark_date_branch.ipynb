{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "# print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEC_EVENT</th>\n",
       "      <th>PAGE</th>\n",
       "      <th>CONTENT_CATEGORY</th>\n",
       "      <th>CONTENT_CATEGORY_TOP</th>\n",
       "      <th>CONTENT_CATEGORY_BOTTOM</th>\n",
       "      <th>SITE_ID</th>\n",
       "      <th>ON_SITE_SEARCH_TERM</th>\n",
       "      <th>USER_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-30 07:35:48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-03-30 07:35:52</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-03-30 07:36:11</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-03-30 07:36:16</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-03-30 07:41:38</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-03-30 07:41:42</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-03-30 07:42:01</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-03-30 07:42:05</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-03-30 07:43:43</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-03-30 07:44:14</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-03-30 07:44:37</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-03-30 07:44:58</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-02-18 13:26:45</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-02-05 14:39:37</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-03-08 10:51:34</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-03-08 10:51:37</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-03-08 10:52:05</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-03-08 10:52:06</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-03-08 10:52:06</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-03-08 10:52:12</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-01-24 12:24:29</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-01-24 12:24:49</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018-01-24 12:24:50</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018-01-24 12:24:50</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018-03-04 18:04:12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018-01-04 08:29:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018-01-04 08:42:20</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018-01-04 08:42:21</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018-01-04 08:42:38</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018-01-04 08:42:38</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936904</th>\n",
       "      <td>2018-09-17 13:41:36</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936905</th>\n",
       "      <td>2018-09-17 13:41:41</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936906</th>\n",
       "      <td>2018-09-17 13:42:01</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936907</th>\n",
       "      <td>2018-09-17 13:43:29</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936908</th>\n",
       "      <td>2018-09-17 13:48:32</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936909</th>\n",
       "      <td>2018-10-07 10:12:21</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936910</th>\n",
       "      <td>2018-10-07 10:12:32</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936911</th>\n",
       "      <td>2018-10-07 10:12:50</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936912</th>\n",
       "      <td>2018-10-07 10:12:52</td>\n",
       "      <td>190</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936913</th>\n",
       "      <td>2018-10-07 10:12:59</td>\n",
       "      <td>190</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936914</th>\n",
       "      <td>2018-10-07 10:13:01</td>\n",
       "      <td>190</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936915</th>\n",
       "      <td>2018-10-07 10:13:04</td>\n",
       "      <td>280</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936916</th>\n",
       "      <td>2018-10-07 10:13:19</td>\n",
       "      <td>190</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936917</th>\n",
       "      <td>2018-10-07 10:18:21</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936918</th>\n",
       "      <td>2018-09-17 13:40:49</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936919</th>\n",
       "      <td>2018-07-05 13:25:02</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936920</th>\n",
       "      <td>2018-07-05 13:25:22</td>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936921</th>\n",
       "      <td>2018-07-05 13:25:34</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936922</th>\n",
       "      <td>2018-07-05 13:26:34</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936923</th>\n",
       "      <td>2018-07-05 13:27:01</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936924</th>\n",
       "      <td>2018-07-05 13:28:03</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936925</th>\n",
       "      <td>2018-07-05 13:28:22</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936926</th>\n",
       "      <td>2018-07-05 13:33:24</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936927</th>\n",
       "      <td>2018-10-31 16:16:04</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936928</th>\n",
       "      <td>2018-10-31 16:17:46</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936929</th>\n",
       "      <td>2018-10-31 16:18:06</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936930</th>\n",
       "      <td>2018-10-31 16:18:35</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936931</th>\n",
       "      <td>2018-10-31 16:23:38</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936932</th>\n",
       "      <td>2018-10-16 10:53:29</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936933</th>\n",
       "      <td>2018-10-16 10:54:09</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17936934 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   FEC_EVENT  PAGE  CONTENT_CATEGORY  CONTENT_CATEGORY_TOP  \\\n",
       "0        2018-03-30 07:35:48     1                 1                     1   \n",
       "1        2018-03-30 07:35:52     2                 2                     2   \n",
       "2        2018-03-30 07:36:11     3                 2                     2   \n",
       "3        2018-03-30 07:36:16     4                 2                     2   \n",
       "4        2018-03-30 07:41:38     5                 2                     2   \n",
       "5        2018-03-30 07:41:42     2                 2                     2   \n",
       "6        2018-03-30 07:42:01     3                 2                     2   \n",
       "7        2018-03-30 07:42:05     4                 2                     2   \n",
       "8        2018-03-30 07:43:43     3                 2                     2   \n",
       "9        2018-03-30 07:44:14     6                 2                     2   \n",
       "10       2018-03-30 07:44:37     7                 2                     2   \n",
       "11       2018-03-30 07:44:58     2                 2                     2   \n",
       "12       2018-02-18 13:26:45     1                 1                     1   \n",
       "13       2018-02-05 14:39:37     8                 3                     3   \n",
       "14       2018-03-08 10:51:34     2                 2                     2   \n",
       "15       2018-03-08 10:51:37     9                 4                     2   \n",
       "16       2018-03-08 10:52:05    10                 4                     2   \n",
       "17       2018-03-08 10:52:06    11                 4                     2   \n",
       "18       2018-03-08 10:52:06    12                 4                     2   \n",
       "19       2018-03-08 10:52:12    13                 4                     2   \n",
       "20       2018-01-24 12:24:29    14                 4                     2   \n",
       "21       2018-01-24 12:24:49    10                 4                     2   \n",
       "22       2018-01-24 12:24:50    11                 4                     2   \n",
       "23       2018-01-24 12:24:50    12                 4                     2   \n",
       "24       2018-03-04 18:04:12     1                 1                     1   \n",
       "25       2018-01-04 08:29:59     1                 1                     1   \n",
       "26       2018-01-04 08:42:20     2                 2                     2   \n",
       "27       2018-01-04 08:42:21    14                 4                     2   \n",
       "28       2018-01-04 08:42:38    11                 4                     2   \n",
       "29       2018-01-04 08:42:38    12                 4                     2   \n",
       "...                      ...   ...               ...                   ...   \n",
       "17936904 2018-09-17 13:41:36     3                 2                     2   \n",
       "17936905 2018-09-17 13:41:41    64                 2                     2   \n",
       "17936906 2018-09-17 13:42:01     3                 2                     2   \n",
       "17936907 2018-09-17 13:43:29    42                 2                     2   \n",
       "17936908 2018-09-17 13:48:32     5                 2                     2   \n",
       "17936909 2018-10-07 10:12:21     2                 2                     2   \n",
       "17936910 2018-10-07 10:12:32     3                 2                     2   \n",
       "17936911 2018-10-07 10:12:50    39                 2                     2   \n",
       "17936912 2018-10-07 10:12:52   190                 2                     2   \n",
       "17936913 2018-10-07 10:12:59   190                 2                     2   \n",
       "17936914 2018-10-07 10:13:01   190                 2                     2   \n",
       "17936915 2018-10-07 10:13:04   280                 2                     2   \n",
       "17936916 2018-10-07 10:13:19   190                 2                     2   \n",
       "17936917 2018-10-07 10:18:21     5                 2                     2   \n",
       "17936918 2018-09-17 13:40:49    40                 1                     1   \n",
       "17936919 2018-07-05 13:25:02     2                 2                     2   \n",
       "17936920 2018-07-05 13:25:22    99                 2                     2   \n",
       "17936921 2018-07-05 13:25:34     2                 2                     2   \n",
       "17936922 2018-07-05 13:26:34     3                 2                     2   \n",
       "17936923 2018-07-05 13:27:01     4                 2                     2   \n",
       "17936924 2018-07-05 13:28:03     3                 2                     2   \n",
       "17936925 2018-07-05 13:28:22     4                 2                     2   \n",
       "17936926 2018-07-05 13:33:24     5                 2                     2   \n",
       "17936927 2018-10-31 16:16:04    40                 1                     1   \n",
       "17936928 2018-10-31 16:17:46     2                 2                     2   \n",
       "17936929 2018-10-31 16:18:06     3                 2                     2   \n",
       "17936930 2018-10-31 16:18:35    23                 2                     2   \n",
       "17936931 2018-10-31 16:23:38     5                 2                     2   \n",
       "17936932 2018-10-16 10:53:29     1                 1                     1   \n",
       "17936933 2018-10-16 10:54:09     2                 2                     2   \n",
       "\n",
       "          CONTENT_CATEGORY_BOTTOM  SITE_ID  ON_SITE_SEARCH_TERM  USER_ID  \n",
       "0                               1        1                    1        0  \n",
       "1                               2        2                    1        0  \n",
       "2                               2        3                    1        0  \n",
       "3                               2        3                    1        0  \n",
       "4                               2        2                    1        0  \n",
       "5                               2        2                    1        0  \n",
       "6                               2        3                    1        0  \n",
       "7                               2        3                    1        0  \n",
       "8                               2        3                    1        0  \n",
       "9                               2        3                    1        0  \n",
       "10                              2        3                    1        0  \n",
       "11                              2        2                    1        0  \n",
       "12                              1        1                    1        0  \n",
       "13                              3        1                    1        0  \n",
       "14                              2        2                    1        0  \n",
       "15                              4        2                    1        0  \n",
       "16                              4        2                    1        0  \n",
       "17                              4        2                    1        0  \n",
       "18                              4        2                    1        0  \n",
       "19                              4        2                    1        0  \n",
       "20                              4        2                    1        0  \n",
       "21                              4        2                    1        0  \n",
       "22                              4        2                    1        0  \n",
       "23                              4        2                    1        0  \n",
       "24                              1        1                    1        0  \n",
       "25                              1        1                    1        0  \n",
       "26                              2        2                    1        0  \n",
       "27                              4        2                    1        0  \n",
       "28                              4        2                    1        0  \n",
       "29                              4        2                    1        0  \n",
       "...                           ...      ...                  ...      ...  \n",
       "17936904                        2        3                    1     4639  \n",
       "17936905                        2        3                    1     4639  \n",
       "17936906                        2        3                    1     4639  \n",
       "17936907                        2        3                    1     4639  \n",
       "17936908                        2        2                    1     4639  \n",
       "17936909                        2        2                    1     4639  \n",
       "17936910                        2        3                    1     4639  \n",
       "17936911                        2        3                    1     4639  \n",
       "17936912                        2        3                    1     4639  \n",
       "17936913                        2        3                    1     4639  \n",
       "17936914                        2        3                    1     4639  \n",
       "17936915                        2        3                    1     4639  \n",
       "17936916                        2        3                    1     4639  \n",
       "17936917                        2        2                    1     4639  \n",
       "17936918                        1        1                    1     4639  \n",
       "17936919                        2        2                    1     4639  \n",
       "17936920                        2        3                    1     4639  \n",
       "17936921                        2        2                    1     4639  \n",
       "17936922                        2        3                    1     4639  \n",
       "17936923                        2        3                    1     4639  \n",
       "17936924                        2        3                    1     4639  \n",
       "17936925                        2        3                    1     4639  \n",
       "17936926                        2        2                    1     4639  \n",
       "17936927                        1        1                    1     4639  \n",
       "17936928                        2        2                    1     4639  \n",
       "17936929                        2        3                    1     4639  \n",
       "17936930                        2        3                    1     4639  \n",
       "17936931                        2        2                    1     4639  \n",
       "17936932                        1        1                    1     4639  \n",
       "17936933                        2        2                    1     4639  \n",
       "\n",
       "[17936934 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./banco-galicia-dataton-2019/pageviews/pageviews.csv\",\n",
    "                   parse_dates=[\"FEC_EVENT\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['YEAR'] = data['FEC_EVENT'].dt.year \n",
    "data['MONTH'] = data['FEC_EVENT'].dt.month \n",
    "data['DAY'] = data['FEC_EVENT'].dt.day \n",
    "data['HOUR'] = data['FEC_EVENT'].dt.hour \n",
    "data['MINUTE'] = data['FEC_EVENT'].dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haciendo PAGE\n",
      "haciendo CONTENT_CATEGORY\n",
      "haciendo CONTENT_CATEGORY_TOP\n",
      "haciendo CONTENT_CATEGORY_BOTTOM\n",
      "haciendo SITE_ID\n",
      "haciendo ON_SITE_SEARCH_TERM\n",
      "haciendo YEAR\n",
      "haciendo MONTH\n",
      "haciendo DAY\n",
      "haciendo HOUR\n",
      "haciendo MINUTE\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "for c in data.drop([\"USER_ID\", \"FEC_EVENT\"], axis=1).columns:\n",
    "    print(\"haciendo\", c)\n",
    "    temp = pd.crosstab(data.USER_ID, data[c])\n",
    "    temp.columns = [c + \"_\" + str(v) for v in temp.columns]\n",
    "    X_test.append(temp.apply(lambda x: x / x.sum(), axis=1))\n",
    "X_test = pd.concat(X_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haciendo PAGE\n",
      "haciendo CONTENT_CATEGORY\n",
      "haciendo CONTENT_CATEGORY_TOP\n",
      "haciendo CONTENT_CATEGORY_BOTTOM\n",
      "haciendo SITE_ID\n",
      "haciendo ON_SITE_SEARCH_TERM\n",
      "haciendo YEAR\n",
      "haciendo MONTH\n",
      "haciendo DAY\n",
      "haciendo HOUR\n",
      "haciendo MINUTE\n"
     ]
    }
   ],
   "source": [
    "data = data[data.FEC_EVENT.dt.month < 10]\n",
    "X_train = []\n",
    "for c in data.drop([\"USER_ID\", \"FEC_EVENT\"], axis=1).columns:\n",
    "    print(\"haciendo\", c)\n",
    "    temp = pd.crosstab(data.USER_ID, data[c])\n",
    "    temp.columns = [c + \"_\" + str(v) for v in temp.columns]\n",
    "    X_train.append(temp.apply(lambda x: x / x.sum(), axis=1))\n",
    "X_train = pd.concat(X_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set(X_train.columns).intersection(set(X_test.columns)))\n",
    "X_train = X_train[features]\n",
    "X_test = X_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PAGE_913'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prev = pd.read_csv(\"./banco-galicia-dataton-2019/conversiones/conversiones.csv\")\n",
    "y_train = pd.Series(0, index=X_train.index)\n",
    "idx = set(y_prev[y_prev.mes >= 10].USER_ID.unique()).intersection(\n",
    "        set(X_train.index))\n",
    "y_train.loc[list(idx)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's binary_logloss: 0.126725\ttraining's auc: 0.797968\tvalid_1's binary_logloss: 0.162279\tvalid_1's auc: 0.72848\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's binary_logloss: 0.117215\ttraining's auc: 0.883574\tvalid_1's binary_logloss: 0.158409\tvalid_1's auc: 0.779506\n",
      "[3]\ttraining's binary_logloss: 0.109588\ttraining's auc: 0.897529\tvalid_1's binary_logloss: 0.155063\tvalid_1's auc: 0.787253\n",
      "[4]\ttraining's binary_logloss: 0.1033\ttraining's auc: 0.914908\tvalid_1's binary_logloss: 0.152742\tvalid_1's auc: 0.798918\n",
      "[5]\ttraining's binary_logloss: 0.0973122\ttraining's auc: 0.930282\tvalid_1's binary_logloss: 0.149534\tvalid_1's auc: 0.815904\n",
      "[6]\ttraining's binary_logloss: 0.0926074\ttraining's auc: 0.93603\tvalid_1's binary_logloss: 0.148416\tvalid_1's auc: 0.813885\n",
      "[7]\ttraining's binary_logloss: 0.0879469\ttraining's auc: 0.941598\tvalid_1's binary_logloss: 0.148082\tvalid_1's auc: 0.810285\n",
      "[8]\ttraining's binary_logloss: 0.0839876\ttraining's auc: 0.943258\tvalid_1's binary_logloss: 0.147399\tvalid_1's auc: 0.810106\n",
      "[9]\ttraining's binary_logloss: 0.0806346\ttraining's auc: 0.951822\tvalid_1's binary_logloss: 0.146939\tvalid_1's auc: 0.807252\n",
      "[10]\ttraining's binary_logloss: 0.0772238\ttraining's auc: 0.958833\tvalid_1's binary_logloss: 0.146689\tvalid_1's auc: 0.810205\n",
      "[11]\ttraining's binary_logloss: 0.0741353\ttraining's auc: 0.961662\tvalid_1's binary_logloss: 0.14672\tvalid_1's auc: 0.807341\n",
      "[12]\ttraining's binary_logloss: 0.0713051\ttraining's auc: 0.963397\tvalid_1's binary_logloss: 0.147433\tvalid_1's auc: 0.801583\n",
      "[13]\ttraining's binary_logloss: 0.0686027\ttraining's auc: 0.96702\tvalid_1's binary_logloss: 0.147281\tvalid_1's auc: 0.801504\n",
      "[14]\ttraining's binary_logloss: 0.0650481\ttraining's auc: 0.980311\tvalid_1's binary_logloss: 0.147097\tvalid_1's auc: 0.802886\n",
      "[15]\ttraining's binary_logloss: 0.0627581\ttraining's auc: 0.98137\tvalid_1's binary_logloss: 0.147669\tvalid_1's auc: 0.799276\n",
      "Early stopping, best iteration is:\n",
      "[5]\ttraining's binary_logloss: 0.0973122\ttraining's auc: 0.930282\tvalid_1's binary_logloss: 0.149534\tvalid_1's auc: 0.815904\n",
      "[1]\ttraining's binary_logloss: 0.127959\ttraining's auc: 0.817812\tvalid_1's binary_logloss: 0.149988\tvalid_1's auc: 0.624795\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's binary_logloss: 0.117006\ttraining's auc: 0.867041\tvalid_1's binary_logloss: 0.149282\tvalid_1's auc: 0.665878\n",
      "[3]\ttraining's binary_logloss: 0.109598\ttraining's auc: 0.891147\tvalid_1's binary_logloss: 0.148953\tvalid_1's auc: 0.70795\n",
      "[4]\ttraining's binary_logloss: 0.102369\ttraining's auc: 0.909879\tvalid_1's binary_logloss: 0.148777\tvalid_1's auc: 0.700796\n",
      "[5]\ttraining's binary_logloss: 0.097064\ttraining's auc: 0.927982\tvalid_1's binary_logloss: 0.148224\tvalid_1's auc: 0.715719\n",
      "[6]\ttraining's binary_logloss: 0.0921354\ttraining's auc: 0.932856\tvalid_1's binary_logloss: 0.147502\tvalid_1's auc: 0.728981\n",
      "[7]\ttraining's binary_logloss: 0.0877662\ttraining's auc: 0.939372\tvalid_1's binary_logloss: 0.147276\tvalid_1's auc: 0.731051\n",
      "[8]\ttraining's binary_logloss: 0.0831887\ttraining's auc: 0.947892\tvalid_1's binary_logloss: 0.146522\tvalid_1's auc: 0.734452\n",
      "[9]\ttraining's binary_logloss: 0.0799511\ttraining's auc: 0.950777\tvalid_1's binary_logloss: 0.146775\tvalid_1's auc: 0.730858\n",
      "[10]\ttraining's binary_logloss: 0.0765884\ttraining's auc: 0.956725\tvalid_1's binary_logloss: 0.146754\tvalid_1's auc: 0.729481\n",
      "[11]\ttraining's binary_logloss: 0.0737006\ttraining's auc: 0.958174\tvalid_1's binary_logloss: 0.146885\tvalid_1's auc: 0.729095\n",
      "[12]\ttraining's binary_logloss: 0.0706703\ttraining's auc: 0.965153\tvalid_1's binary_logloss: 0.1476\tvalid_1's auc: 0.730687\n",
      "[13]\ttraining's binary_logloss: 0.0678042\ttraining's auc: 0.978597\tvalid_1's binary_logloss: 0.147682\tvalid_1's auc: 0.717141\n",
      "[14]\ttraining's binary_logloss: 0.0650301\ttraining's auc: 0.983812\tvalid_1's binary_logloss: 0.148409\tvalid_1's auc: 0.71523\n",
      "[15]\ttraining's binary_logloss: 0.0626184\ttraining's auc: 0.985092\tvalid_1's binary_logloss: 0.148776\tvalid_1's auc: 0.715412\n",
      "[16]\ttraining's binary_logloss: 0.0599761\ttraining's auc: 0.988714\tvalid_1's binary_logloss: 0.149478\tvalid_1's auc: 0.715958\n",
      "[17]\ttraining's binary_logloss: 0.0579315\ttraining's auc: 0.989497\tvalid_1's binary_logloss: 0.14883\tvalid_1's auc: 0.724329\n",
      "[18]\ttraining's binary_logloss: 0.0559931\ttraining's auc: 0.991744\tvalid_1's binary_logloss: 0.149432\tvalid_1's auc: 0.72442\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's binary_logloss: 0.0831887\ttraining's auc: 0.947892\tvalid_1's binary_logloss: 0.146522\tvalid_1's auc: 0.734452\n",
      "[1]\ttraining's binary_logloss: 0.131065\ttraining's auc: 0.820199\tvalid_1's binary_logloss: 0.131389\tvalid_1's auc: 0.717029\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's binary_logloss: 0.120706\ttraining's auc: 0.879446\tvalid_1's binary_logloss: 0.127298\tvalid_1's auc: 0.813445\n",
      "[3]\ttraining's binary_logloss: 0.112264\ttraining's auc: 0.896006\tvalid_1's binary_logloss: 0.125706\tvalid_1's auc: 0.824431\n",
      "[4]\ttraining's binary_logloss: 0.106157\ttraining's auc: 0.915124\tvalid_1's binary_logloss: 0.123472\tvalid_1's auc: 0.826475\n",
      "[5]\ttraining's binary_logloss: 0.100109\ttraining's auc: 0.92288\tvalid_1's binary_logloss: 0.121687\tvalid_1's auc: 0.832583\n",
      "[6]\ttraining's binary_logloss: 0.095192\ttraining's auc: 0.93415\tvalid_1's binary_logloss: 0.12087\tvalid_1's auc: 0.829037\n",
      "[7]\ttraining's binary_logloss: 0.090735\ttraining's auc: 0.937031\tvalid_1's binary_logloss: 0.119737\tvalid_1's auc: 0.834653\n",
      "[8]\ttraining's binary_logloss: 0.0865588\ttraining's auc: 0.943011\tvalid_1's binary_logloss: 0.118699\tvalid_1's auc: 0.839506\n",
      "[9]\ttraining's binary_logloss: 0.082957\ttraining's auc: 0.948437\tvalid_1's binary_logloss: 0.118617\tvalid_1's auc: 0.836115\n",
      "[10]\ttraining's binary_logloss: 0.0795203\ttraining's auc: 0.956203\tvalid_1's binary_logloss: 0.117018\tvalid_1's auc: 0.841524\n",
      "[11]\ttraining's binary_logloss: 0.076427\ttraining's auc: 0.958002\tvalid_1's binary_logloss: 0.115887\tvalid_1's auc: 0.843595\n",
      "[12]\ttraining's binary_logloss: 0.0738225\ttraining's auc: 0.962317\tvalid_1's binary_logloss: 0.115235\tvalid_1's auc: 0.844138\n",
      "[13]\ttraining's binary_logloss: 0.0710746\ttraining's auc: 0.962818\tvalid_1's binary_logloss: 0.114996\tvalid_1's auc: 0.841822\n",
      "[14]\ttraining's binary_logloss: 0.0680691\ttraining's auc: 0.974546\tvalid_1's binary_logloss: 0.113567\tvalid_1's auc: 0.859964\n",
      "[15]\ttraining's binary_logloss: 0.0654553\ttraining's auc: 0.981907\tvalid_1's binary_logloss: 0.113124\tvalid_1's auc: 0.853028\n",
      "[16]\ttraining's binary_logloss: 0.0629042\ttraining's auc: 0.985306\tvalid_1's binary_logloss: 0.112988\tvalid_1's auc: 0.853131\n",
      "[17]\ttraining's binary_logloss: 0.060351\ttraining's auc: 0.990777\tvalid_1's binary_logloss: 0.11301\tvalid_1's auc: 0.853157\n",
      "[18]\ttraining's binary_logloss: 0.0581639\ttraining's auc: 0.992758\tvalid_1's binary_logloss: 0.11289\tvalid_1's auc: 0.85832\n",
      "[19]\ttraining's binary_logloss: 0.0562237\ttraining's auc: 0.993784\tvalid_1's binary_logloss: 0.112259\tvalid_1's auc: 0.859627\n",
      "[20]\ttraining's binary_logloss: 0.0541912\ttraining's auc: 0.994883\tvalid_1's binary_logloss: 0.111125\tvalid_1's auc: 0.866641\n",
      "[21]\ttraining's binary_logloss: 0.0522189\ttraining's auc: 0.996938\tvalid_1's binary_logloss: 0.111044\tvalid_1's auc: 0.866149\n",
      "[22]\ttraining's binary_logloss: 0.0502757\ttraining's auc: 0.99831\tvalid_1's binary_logloss: 0.110958\tvalid_1's auc: 0.864467\n",
      "[23]\ttraining's binary_logloss: 0.0485496\ttraining's auc: 0.999068\tvalid_1's binary_logloss: 0.111109\tvalid_1's auc: 0.865295\n",
      "[24]\ttraining's binary_logloss: 0.0468772\ttraining's auc: 0.999283\tvalid_1's binary_logloss: 0.111463\tvalid_1's auc: 0.864286\n",
      "[25]\ttraining's binary_logloss: 0.0453808\ttraining's auc: 0.999611\tvalid_1's binary_logloss: 0.110958\tvalid_1's auc: 0.866537\n",
      "[26]\ttraining's binary_logloss: 0.0439096\ttraining's auc: 0.999814\tvalid_1's binary_logloss: 0.110702\tvalid_1's auc: 0.86778\n",
      "[27]\ttraining's binary_logloss: 0.0424681\ttraining's auc: 0.999875\tvalid_1's binary_logloss: 0.111013\tvalid_1's auc: 0.866486\n",
      "[28]\ttraining's binary_logloss: 0.0411808\ttraining's auc: 0.999923\tvalid_1's binary_logloss: 0.11078\tvalid_1's auc: 0.865709\n",
      "[29]\ttraining's binary_logloss: 0.0398313\ttraining's auc: 0.999949\tvalid_1's binary_logloss: 0.110378\tvalid_1's auc: 0.868349\n",
      "[30]\ttraining's binary_logloss: 0.0385444\ttraining's auc: 0.999961\tvalid_1's binary_logloss: 0.110526\tvalid_1's auc: 0.866822\n",
      "[31]\ttraining's binary_logloss: 0.0373294\ttraining's auc: 0.999973\tvalid_1's binary_logloss: 0.110248\tvalid_1's auc: 0.868659\n",
      "[32]\ttraining's binary_logloss: 0.0361142\ttraining's auc: 0.999981\tvalid_1's binary_logloss: 0.11057\tvalid_1's auc: 0.867728\n",
      "[33]\ttraining's binary_logloss: 0.0349755\ttraining's auc: 0.99999\tvalid_1's binary_logloss: 0.111047\tvalid_1's auc: 0.867236\n",
      "[34]\ttraining's binary_logloss: 0.0339194\ttraining's auc: 0.999995\tvalid_1's binary_logloss: 0.111725\tvalid_1's auc: 0.865864\n",
      "[35]\ttraining's binary_logloss: 0.0328879\ttraining's auc: 0.999996\tvalid_1's binary_logloss: 0.11117\tvalid_1's auc: 0.867469\n",
      "[36]\ttraining's binary_logloss: 0.031918\ttraining's auc: 0.999999\tvalid_1's binary_logloss: 0.110916\tvalid_1's auc: 0.868996\n",
      "[37]\ttraining's binary_logloss: 0.0309841\ttraining's auc: 0.999999\tvalid_1's binary_logloss: 0.111473\tvalid_1's auc: 0.867572\n",
      "[38]\ttraining's binary_logloss: 0.0301047\ttraining's auc: 1\tvalid_1's binary_logloss: 0.11161\tvalid_1's auc: 0.869125\n",
      "[39]\ttraining's binary_logloss: 0.029207\ttraining's auc: 1\tvalid_1's binary_logloss: 0.11143\tvalid_1's auc: 0.871687\n",
      "[40]\ttraining's binary_logloss: 0.0283382\ttraining's auc: 1\tvalid_1's binary_logloss: 0.110915\tvalid_1's auc: 0.873965\n",
      "[41]\ttraining's binary_logloss: 0.027511\ttraining's auc: 1\tvalid_1's binary_logloss: 0.111011\tvalid_1's auc: 0.875699\n",
      "Early stopping, best iteration is:\n",
      "[31]\ttraining's binary_logloss: 0.0373294\ttraining's auc: 0.999973\tvalid_1's binary_logloss: 0.110248\tvalid_1's auc: 0.868659\n",
      "[1]\ttraining's binary_logloss: 0.13083\ttraining's auc: 0.84711\tvalid_1's binary_logloss: 0.131952\tvalid_1's auc: 0.786982\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's binary_logloss: 0.119826\ttraining's auc: 0.864764\tvalid_1's binary_logloss: 0.129758\tvalid_1's auc: 0.802811\n",
      "[3]\ttraining's binary_logloss: 0.111953\ttraining's auc: 0.879129\tvalid_1's binary_logloss: 0.127897\tvalid_1's auc: 0.811008\n",
      "[4]\ttraining's binary_logloss: 0.105344\ttraining's auc: 0.891693\tvalid_1's binary_logloss: 0.126643\tvalid_1's auc: 0.811612\n",
      "[5]\ttraining's binary_logloss: 0.0996316\ttraining's auc: 0.911609\tvalid_1's binary_logloss: 0.125383\tvalid_1's auc: 0.815075\n",
      "[6]\ttraining's binary_logloss: 0.094521\ttraining's auc: 0.92784\tvalid_1's binary_logloss: 0.12489\tvalid_1's auc: 0.798718\n",
      "[7]\ttraining's binary_logloss: 0.0899246\ttraining's auc: 0.93989\tvalid_1's binary_logloss: 0.124076\tvalid_1's auc: 0.798731\n",
      "[8]\ttraining's binary_logloss: 0.0860513\ttraining's auc: 0.943741\tvalid_1's binary_logloss: 0.124333\tvalid_1's auc: 0.798063\n",
      "[9]\ttraining's binary_logloss: 0.0824943\ttraining's auc: 0.955249\tvalid_1's binary_logloss: 0.123506\tvalid_1's auc: 0.797283\n",
      "[10]\ttraining's binary_logloss: 0.0792749\ttraining's auc: 0.958636\tvalid_1's binary_logloss: 0.123379\tvalid_1's auc: 0.794878\n",
      "[11]\ttraining's binary_logloss: 0.0761231\ttraining's auc: 0.966144\tvalid_1's binary_logloss: 0.122641\tvalid_1's auc: 0.791239\n",
      "[12]\ttraining's binary_logloss: 0.0727988\ttraining's auc: 0.972347\tvalid_1's binary_logloss: 0.122124\tvalid_1's auc: 0.794298\n",
      "[13]\ttraining's binary_logloss: 0.0700633\ttraining's auc: 0.974756\tvalid_1's binary_logloss: 0.12319\tvalid_1's auc: 0.78804\n",
      "[14]\ttraining's binary_logloss: 0.0675479\ttraining's auc: 0.976701\tvalid_1's binary_logloss: 0.123455\tvalid_1's auc: 0.790445\n",
      "[15]\ttraining's binary_logloss: 0.0651142\ttraining's auc: 0.978106\tvalid_1's binary_logloss: 0.123772\tvalid_1's auc: 0.79892\n",
      "Early stopping, best iteration is:\n",
      "[5]\ttraining's binary_logloss: 0.0996316\ttraining's auc: 0.911609\tvalid_1's binary_logloss: 0.125383\tvalid_1's auc: 0.815075\n",
      "[1]\ttraining's binary_logloss: 0.126345\ttraining's auc: 0.84041\tvalid_1's binary_logloss: 0.162632\tvalid_1's auc: 0.760716\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's binary_logloss: 0.115893\ttraining's auc: 0.876256\tvalid_1's binary_logloss: 0.157779\tvalid_1's auc: 0.776966\n",
      "[3]\ttraining's binary_logloss: 0.108566\ttraining's auc: 0.879592\tvalid_1's binary_logloss: 0.156297\tvalid_1's auc: 0.786611\n",
      "[4]\ttraining's binary_logloss: 0.10165\ttraining's auc: 0.905609\tvalid_1's binary_logloss: 0.154027\tvalid_1's auc: 0.799149\n",
      "[5]\ttraining's binary_logloss: 0.0964258\ttraining's auc: 0.92034\tvalid_1's binary_logloss: 0.153857\tvalid_1's auc: 0.795758\n",
      "[6]\ttraining's binary_logloss: 0.091625\ttraining's auc: 0.923565\tvalid_1's binary_logloss: 0.153859\tvalid_1's auc: 0.791862\n",
      "[7]\ttraining's binary_logloss: 0.0873719\ttraining's auc: 0.933085\tvalid_1's binary_logloss: 0.152415\tvalid_1's auc: 0.78775\n",
      "[8]\ttraining's binary_logloss: 0.0834752\ttraining's auc: 0.937899\tvalid_1's binary_logloss: 0.151387\tvalid_1's auc: 0.79911\n",
      "[9]\ttraining's binary_logloss: 0.0798322\ttraining's auc: 0.9437\tvalid_1's binary_logloss: 0.151609\tvalid_1's auc: 0.799626\n",
      "[10]\ttraining's binary_logloss: 0.076698\ttraining's auc: 0.948264\tvalid_1's binary_logloss: 0.150793\tvalid_1's auc: 0.79834\n",
      "[11]\ttraining's binary_logloss: 0.0734505\ttraining's auc: 0.956926\tvalid_1's binary_logloss: 0.149557\tvalid_1's auc: 0.803318\n",
      "[12]\ttraining's binary_logloss: 0.0701654\ttraining's auc: 0.972921\tvalid_1's binary_logloss: 0.149864\tvalid_1's auc: 0.814804\n",
      "[13]\ttraining's binary_logloss: 0.0675488\ttraining's auc: 0.976027\tvalid_1's binary_logloss: 0.149809\tvalid_1's auc: 0.813304\n",
      "[14]\ttraining's binary_logloss: 0.06472\ttraining's auc: 0.983574\tvalid_1's binary_logloss: 0.149796\tvalid_1's auc: 0.806241\n",
      "[15]\ttraining's binary_logloss: 0.0616045\ttraining's auc: 0.990121\tvalid_1's binary_logloss: 0.149264\tvalid_1's auc: 0.81232\n",
      "[16]\ttraining's binary_logloss: 0.0591393\ttraining's auc: 0.993599\tvalid_1's binary_logloss: 0.149111\tvalid_1's auc: 0.815846\n",
      "[17]\ttraining's binary_logloss: 0.0567614\ttraining's auc: 0.995719\tvalid_1's binary_logloss: 0.149724\tvalid_1's auc: 0.817707\n",
      "[18]\ttraining's binary_logloss: 0.0545972\ttraining's auc: 0.996797\tvalid_1's binary_logloss: 0.149637\tvalid_1's auc: 0.816908\n",
      "[19]\ttraining's binary_logloss: 0.0524995\ttraining's auc: 0.997976\tvalid_1's binary_logloss: 0.149336\tvalid_1's auc: 0.819285\n",
      "[20]\ttraining's binary_logloss: 0.0507026\ttraining's auc: 0.998309\tvalid_1's binary_logloss: 0.148607\tvalid_1's auc: 0.823591\n",
      "[21]\ttraining's binary_logloss: 0.0489686\ttraining's auc: 0.998831\tvalid_1's binary_logloss: 0.148214\tvalid_1's auc: 0.824546\n",
      "[22]\ttraining's binary_logloss: 0.0473373\ttraining's auc: 0.999011\tvalid_1's binary_logloss: 0.148673\tvalid_1's auc: 0.823845\n",
      "[23]\ttraining's binary_logloss: 0.0454421\ttraining's auc: 0.999459\tvalid_1's binary_logloss: 0.147773\tvalid_1's auc: 0.827819\n",
      "[24]\ttraining's binary_logloss: 0.0439462\ttraining's auc: 0.999603\tvalid_1's binary_logloss: 0.147329\tvalid_1's auc: 0.830956\n",
      "[25]\ttraining's binary_logloss: 0.0425211\ttraining's auc: 0.999777\tvalid_1's binary_logloss: 0.147503\tvalid_1's auc: 0.83193\n",
      "[26]\ttraining's binary_logloss: 0.0411434\ttraining's auc: 0.999851\tvalid_1's binary_logloss: 0.147385\tvalid_1's auc: 0.832476\n",
      "[27]\ttraining's binary_logloss: 0.0399193\ttraining's auc: 0.999884\tvalid_1's binary_logloss: 0.147581\tvalid_1's auc: 0.83269\n",
      "[28]\ttraining's binary_logloss: 0.0387028\ttraining's auc: 0.999918\tvalid_1's binary_logloss: 0.148353\tvalid_1's auc: 0.830489\n",
      "[29]\ttraining's binary_logloss: 0.0374943\ttraining's auc: 0.999946\tvalid_1's binary_logloss: 0.148191\tvalid_1's auc: 0.832164\n",
      "[30]\ttraining's binary_logloss: 0.0363616\ttraining's auc: 0.99997\tvalid_1's binary_logloss: 0.148114\tvalid_1's auc: 0.832983\n",
      "[31]\ttraining's binary_logloss: 0.0351589\ttraining's auc: 0.999978\tvalid_1's binary_logloss: 0.148149\tvalid_1's auc: 0.832593\n",
      "[32]\ttraining's binary_logloss: 0.0340023\ttraining's auc: 0.999987\tvalid_1's binary_logloss: 0.149129\tvalid_1's auc: 0.830294\n",
      "[33]\ttraining's binary_logloss: 0.0328986\ttraining's auc: 0.99999\tvalid_1's binary_logloss: 0.149577\tvalid_1's auc: 0.829651\n",
      "[34]\ttraining's binary_logloss: 0.0318569\ttraining's auc: 0.999991\tvalid_1's binary_logloss: 0.149755\tvalid_1's auc: 0.828326\n",
      "Early stopping, best iteration is:\n",
      "[24]\ttraining's binary_logloss: 0.0439462\ttraining's auc: 0.999603\tvalid_1's binary_logloss: 0.147329\tvalid_1's auc: 0.830956\n",
      "[1]\ttraining's binary_logloss: 0.128751\ttraining's auc: 0.811806\tvalid_1's binary_logloss: 0.150847\tvalid_1's auc: 0.655449\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's binary_logloss: 0.118739\ttraining's auc: 0.871835\tvalid_1's binary_logloss: 0.149441\tvalid_1's auc: 0.734806\n",
      "[3]\ttraining's binary_logloss: 0.110307\ttraining's auc: 0.883927\tvalid_1's binary_logloss: 0.14791\tvalid_1's auc: 0.740726\n",
      "[4]\ttraining's binary_logloss: 0.103591\ttraining's auc: 0.903285\tvalid_1's binary_logloss: 0.146346\tvalid_1's auc: 0.735039\n",
      "[5]\ttraining's binary_logloss: 0.0980944\ttraining's auc: 0.911311\tvalid_1's binary_logloss: 0.145305\tvalid_1's auc: 0.719945\n",
      "[6]\ttraining's binary_logloss: 0.0932551\ttraining's auc: 0.923708\tvalid_1's binary_logloss: 0.145015\tvalid_1's auc: 0.720867\n",
      "[7]\ttraining's binary_logloss: 0.0884852\ttraining's auc: 0.934282\tvalid_1's binary_logloss: 0.143832\tvalid_1's auc: 0.732585\n",
      "[8]\ttraining's binary_logloss: 0.0845693\ttraining's auc: 0.937749\tvalid_1's binary_logloss: 0.143264\tvalid_1's auc: 0.731152\n",
      "[9]\ttraining's binary_logloss: 0.0811337\ttraining's auc: 0.949206\tvalid_1's binary_logloss: 0.142082\tvalid_1's auc: 0.759763\n",
      "[10]\ttraining's binary_logloss: 0.0780085\ttraining's auc: 0.953153\tvalid_1's binary_logloss: 0.140997\tvalid_1's auc: 0.776834\n",
      "[11]\ttraining's binary_logloss: 0.0750849\ttraining's auc: 0.95563\tvalid_1's binary_logloss: 0.140796\tvalid_1's auc: 0.782154\n",
      "[12]\ttraining's binary_logloss: 0.0720284\ttraining's auc: 0.968341\tvalid_1's binary_logloss: 0.140887\tvalid_1's auc: 0.78151\n",
      "[13]\ttraining's binary_logloss: 0.0692744\ttraining's auc: 0.96902\tvalid_1's binary_logloss: 0.141027\tvalid_1's auc: 0.779833\n",
      "[14]\ttraining's binary_logloss: 0.0666039\ttraining's auc: 0.974464\tvalid_1's binary_logloss: 0.141124\tvalid_1's auc: 0.782731\n",
      "[15]\ttraining's binary_logloss: 0.0641615\ttraining's auc: 0.976892\tvalid_1's binary_logloss: 0.140143\tvalid_1's auc: 0.785175\n",
      "[16]\ttraining's binary_logloss: 0.0617484\ttraining's auc: 0.980108\tvalid_1's binary_logloss: 0.140072\tvalid_1's auc: 0.784842\n",
      "[17]\ttraining's binary_logloss: 0.0591838\ttraining's auc: 0.986169\tvalid_1's binary_logloss: 0.14061\tvalid_1's auc: 0.784608\n",
      "[18]\ttraining's binary_logloss: 0.0567761\ttraining's auc: 0.99238\tvalid_1's binary_logloss: 0.140699\tvalid_1's auc: 0.782109\n",
      "[19]\ttraining's binary_logloss: 0.0545537\ttraining's auc: 0.994917\tvalid_1's binary_logloss: 0.14095\tvalid_1's auc: 0.777445\n",
      "[20]\ttraining's binary_logloss: 0.0527575\ttraining's auc: 0.995534\tvalid_1's binary_logloss: 0.141527\tvalid_1's auc: 0.774735\n",
      "[21]\ttraining's binary_logloss: 0.0509795\ttraining's auc: 0.996852\tvalid_1's binary_logloss: 0.142023\tvalid_1's auc: 0.766849\n",
      "[22]\ttraining's binary_logloss: 0.0490688\ttraining's auc: 0.99814\tvalid_1's binary_logloss: 0.142751\tvalid_1's auc: 0.763417\n",
      "[23]\ttraining's binary_logloss: 0.047349\ttraining's auc: 0.998668\tvalid_1's binary_logloss: 0.143131\tvalid_1's auc: 0.764316\n",
      "[24]\ttraining's binary_logloss: 0.0458335\ttraining's auc: 0.999411\tvalid_1's binary_logloss: 0.1438\tvalid_1's auc: 0.76134\n",
      "[25]\ttraining's binary_logloss: 0.0443137\ttraining's auc: 0.999665\tvalid_1's binary_logloss: 0.143043\tvalid_1's auc: 0.768071\n",
      "Early stopping, best iteration is:\n",
      "[15]\ttraining's binary_logloss: 0.0641615\ttraining's auc: 0.976892\tvalid_1's binary_logloss: 0.140143\tvalid_1's auc: 0.785175\n",
      "[1]\ttraining's binary_logloss: 0.129561\ttraining's auc: 0.841136\tvalid_1's binary_logloss: 0.139209\tvalid_1's auc: 0.784998\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's binary_logloss: 0.118757\ttraining's auc: 0.870156\tvalid_1's binary_logloss: 0.132873\tvalid_1's auc: 0.808947\n",
      "[3]\ttraining's binary_logloss: 0.110746\ttraining's auc: 0.89291\tvalid_1's binary_logloss: 0.131288\tvalid_1's auc: 0.802963\n",
      "[4]\ttraining's binary_logloss: 0.104402\ttraining's auc: 0.897578\tvalid_1's binary_logloss: 0.129567\tvalid_1's auc: 0.801724\n",
      "[5]\ttraining's binary_logloss: 0.0986266\ttraining's auc: 0.91345\tvalid_1's binary_logloss: 0.128956\tvalid_1's auc: 0.80685\n",
      "[6]\ttraining's binary_logloss: 0.0935554\ttraining's auc: 0.921544\tvalid_1's binary_logloss: 0.127973\tvalid_1's auc: 0.821246\n",
      "[7]\ttraining's binary_logloss: 0.0893407\ttraining's auc: 0.932882\tvalid_1's binary_logloss: 0.127379\tvalid_1's auc: 0.824386\n",
      "[8]\ttraining's binary_logloss: 0.0849871\ttraining's auc: 0.939347\tvalid_1's binary_logloss: 0.12747\tvalid_1's auc: 0.817531\n",
      "[9]\ttraining's binary_logloss: 0.0812629\ttraining's auc: 0.943178\tvalid_1's binary_logloss: 0.12709\tvalid_1's auc: 0.825379\n",
      "[10]\ttraining's binary_logloss: 0.0780707\ttraining's auc: 0.947753\tvalid_1's binary_logloss: 0.127206\tvalid_1's auc: 0.818524\n",
      "[11]\ttraining's binary_logloss: 0.075172\ttraining's auc: 0.954087\tvalid_1's binary_logloss: 0.126868\tvalid_1's auc: 0.816439\n",
      "[12]\ttraining's binary_logloss: 0.0722812\ttraining's auc: 0.96243\tvalid_1's binary_logloss: 0.126418\tvalid_1's auc: 0.820523\n",
      "[13]\ttraining's binary_logloss: 0.0693682\ttraining's auc: 0.969895\tvalid_1's binary_logloss: 0.126093\tvalid_1's auc: 0.816427\n",
      "[14]\ttraining's binary_logloss: 0.0666176\ttraining's auc: 0.977187\tvalid_1's binary_logloss: 0.126415\tvalid_1's auc: 0.81346\n",
      "[15]\ttraining's binary_logloss: 0.0637341\ttraining's auc: 0.984573\tvalid_1's binary_logloss: 0.126265\tvalid_1's auc: 0.814796\n",
      "[16]\ttraining's binary_logloss: 0.0610916\ttraining's auc: 0.987698\tvalid_1's binary_logloss: 0.125887\tvalid_1's auc: 0.817224\n",
      "[17]\ttraining's binary_logloss: 0.0587999\ttraining's auc: 0.990936\tvalid_1's binary_logloss: 0.125211\tvalid_1's auc: 0.817715\n",
      "[18]\ttraining's binary_logloss: 0.0567172\ttraining's auc: 0.991327\tvalid_1's binary_logloss: 0.124697\tvalid_1's auc: 0.819211\n",
      "[19]\ttraining's binary_logloss: 0.0544367\ttraining's auc: 0.994825\tvalid_1's binary_logloss: 0.124361\tvalid_1's auc: 0.822853\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttraining's binary_logloss: 0.0812629\ttraining's auc: 0.943178\tvalid_1's binary_logloss: 0.12709\tvalid_1's auc: 0.825379\n",
      "[1]\ttraining's binary_logloss: 0.130964\ttraining's auc: 0.837462\tvalid_1's binary_logloss: 0.136386\tvalid_1's auc: 0.746622\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's binary_logloss: 0.120425\ttraining's auc: 0.861419\tvalid_1's binary_logloss: 0.133689\tvalid_1's auc: 0.758961\n",
      "[3]\ttraining's binary_logloss: 0.112071\ttraining's auc: 0.895768\tvalid_1's binary_logloss: 0.13238\tvalid_1's auc: 0.776316\n",
      "[4]\ttraining's binary_logloss: 0.105379\ttraining's auc: 0.917575\tvalid_1's binary_logloss: 0.130172\tvalid_1's auc: 0.784256\n",
      "[5]\ttraining's binary_logloss: 0.099764\ttraining's auc: 0.92734\tvalid_1's binary_logloss: 0.129472\tvalid_1's auc: 0.779908\n",
      "[6]\ttraining's binary_logloss: 0.0943409\ttraining's auc: 0.937179\tvalid_1's binary_logloss: 0.128421\tvalid_1's auc: 0.786966\n",
      "[7]\ttraining's binary_logloss: 0.0898441\ttraining's auc: 0.940201\tvalid_1's binary_logloss: 0.126883\tvalid_1's auc: 0.790028\n",
      "[8]\ttraining's binary_logloss: 0.0858761\ttraining's auc: 0.941646\tvalid_1's binary_logloss: 0.125827\tvalid_1's auc: 0.79265\n",
      "[9]\ttraining's binary_logloss: 0.0826371\ttraining's auc: 0.948565\tvalid_1's binary_logloss: 0.125601\tvalid_1's auc: 0.790104\n",
      "[10]\ttraining's binary_logloss: 0.0793899\ttraining's auc: 0.951359\tvalid_1's binary_logloss: 0.125641\tvalid_1's auc: 0.790381\n",
      "[11]\ttraining's binary_logloss: 0.0764055\ttraining's auc: 0.960558\tvalid_1's binary_logloss: 0.124725\tvalid_1's auc: 0.799909\n",
      "[12]\ttraining's binary_logloss: 0.0735888\ttraining's auc: 0.961934\tvalid_1's binary_logloss: 0.124063\tvalid_1's auc: 0.802304\n",
      "[13]\ttraining's binary_logloss: 0.0706439\ttraining's auc: 0.970467\tvalid_1's binary_logloss: 0.123216\tvalid_1's auc: 0.824398\n",
      "[14]\ttraining's binary_logloss: 0.0681179\ttraining's auc: 0.972808\tvalid_1's binary_logloss: 0.123311\tvalid_1's auc: 0.822986\n",
      "[15]\ttraining's binary_logloss: 0.0655899\ttraining's auc: 0.976602\tvalid_1's binary_logloss: 0.123355\tvalid_1's auc: 0.819192\n",
      "[16]\ttraining's binary_logloss: 0.0630283\ttraining's auc: 0.981675\tvalid_1's binary_logloss: 0.122815\tvalid_1's auc: 0.818928\n",
      "[17]\ttraining's binary_logloss: 0.0603619\ttraining's auc: 0.98693\tvalid_1's binary_logloss: 0.122523\tvalid_1's auc: 0.820276\n",
      "[18]\ttraining's binary_logloss: 0.0581113\ttraining's auc: 0.990122\tvalid_1's binary_logloss: 0.121735\tvalid_1's auc: 0.819041\n",
      "[19]\ttraining's binary_logloss: 0.0560518\ttraining's auc: 0.99179\tvalid_1's binary_logloss: 0.121449\tvalid_1's auc: 0.821587\n",
      "[20]\ttraining's binary_logloss: 0.0542017\ttraining's auc: 0.994109\tvalid_1's binary_logloss: 0.121512\tvalid_1's auc: 0.818235\n",
      "[21]\ttraining's binary_logloss: 0.0521877\ttraining's auc: 0.996076\tvalid_1's binary_logloss: 0.121802\tvalid_1's auc: 0.815437\n",
      "[22]\ttraining's binary_logloss: 0.0504611\ttraining's auc: 0.997072\tvalid_1's binary_logloss: 0.121913\tvalid_1's auc: 0.815008\n",
      "[23]\ttraining's binary_logloss: 0.0489229\ttraining's auc: 0.997325\tvalid_1's binary_logloss: 0.121594\tvalid_1's auc: 0.816747\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttraining's binary_logloss: 0.0706439\ttraining's auc: 0.970467\tvalid_1's binary_logloss: 0.123216\tvalid_1's auc: 0.824398\n",
      "[1]\ttraining's binary_logloss: 0.131186\ttraining's auc: 0.834506\tvalid_1's binary_logloss: 0.127053\tvalid_1's auc: 0.84311\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's binary_logloss: 0.120935\ttraining's auc: 0.865121\tvalid_1's binary_logloss: 0.12213\tvalid_1's auc: 0.881955\n",
      "[3]\ttraining's binary_logloss: 0.113407\ttraining's auc: 0.878829\tvalid_1's binary_logloss: 0.119115\tvalid_1's auc: 0.87451\n",
      "[4]\ttraining's binary_logloss: 0.107145\ttraining's auc: 0.899103\tvalid_1's binary_logloss: 0.116869\tvalid_1's auc: 0.868819\n",
      "[5]\ttraining's binary_logloss: 0.100822\ttraining's auc: 0.917867\tvalid_1's binary_logloss: 0.11445\tvalid_1's auc: 0.875785\n",
      "[6]\ttraining's binary_logloss: 0.096014\ttraining's auc: 0.927221\tvalid_1's binary_logloss: 0.113018\tvalid_1's auc: 0.873331\n",
      "[7]\ttraining's binary_logloss: 0.0914618\ttraining's auc: 0.932448\tvalid_1's binary_logloss: 0.111544\tvalid_1's auc: 0.87355\n",
      "[8]\ttraining's binary_logloss: 0.0877696\ttraining's auc: 0.940247\tvalid_1's binary_logloss: 0.111025\tvalid_1's auc: 0.871644\n",
      "[9]\ttraining's binary_logloss: 0.0838525\ttraining's auc: 0.947797\tvalid_1's binary_logloss: 0.110762\tvalid_1's auc: 0.868449\n",
      "[10]\ttraining's binary_logloss: 0.0804889\ttraining's auc: 0.949222\tvalid_1's binary_logloss: 0.109714\tvalid_1's auc: 0.869286\n",
      "[11]\ttraining's binary_logloss: 0.0774307\ttraining's auc: 0.956928\tvalid_1's binary_logloss: 0.109291\tvalid_1's auc: 0.867339\n",
      "[12]\ttraining's binary_logloss: 0.0745629\ttraining's auc: 0.963171\tvalid_1's binary_logloss: 0.1092\tvalid_1's auc: 0.864582\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's binary_logloss: 0.120935\ttraining's auc: 0.865121\tvalid_1's binary_logloss: 0.12213\tvalid_1's auc: 0.881955\n",
      "[1]\ttraining's binary_logloss: 0.131421\ttraining's auc: 0.847506\tvalid_1's binary_logloss: 0.141079\tvalid_1's auc: 0.736063\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's binary_logloss: 0.121108\ttraining's auc: 0.868529\tvalid_1's binary_logloss: 0.137558\tvalid_1's auc: 0.779193\n",
      "[3]\ttraining's binary_logloss: 0.113298\ttraining's auc: 0.88859\tvalid_1's binary_logloss: 0.136463\tvalid_1's auc: 0.782863\n",
      "[4]\ttraining's binary_logloss: 0.10699\ttraining's auc: 0.894182\tvalid_1's binary_logloss: 0.135924\tvalid_1's auc: 0.771375\n",
      "[5]\ttraining's binary_logloss: 0.101428\ttraining's auc: 0.912239\tvalid_1's binary_logloss: 0.134703\tvalid_1's auc: 0.770945\n",
      "[6]\ttraining's binary_logloss: 0.09665\ttraining's auc: 0.923895\tvalid_1's binary_logloss: 0.134524\tvalid_1's auc: 0.766834\n",
      "[7]\ttraining's binary_logloss: 0.0916528\ttraining's auc: 0.932885\tvalid_1's binary_logloss: 0.133191\tvalid_1's auc: 0.773106\n",
      "[8]\ttraining's binary_logloss: 0.0882122\ttraining's auc: 0.942173\tvalid_1's binary_logloss: 0.132969\tvalid_1's auc: 0.773265\n",
      "[9]\ttraining's binary_logloss: 0.0846824\ttraining's auc: 0.946935\tvalid_1's binary_logloss: 0.132683\tvalid_1's auc: 0.770798\n",
      "[10]\ttraining's binary_logloss: 0.081271\ttraining's auc: 0.952365\tvalid_1's binary_logloss: 0.131633\tvalid_1's auc: 0.780936\n",
      "[11]\ttraining's binary_logloss: 0.0779005\ttraining's auc: 0.956037\tvalid_1's binary_logloss: 0.131166\tvalid_1's auc: 0.79418\n",
      "[12]\ttraining's binary_logloss: 0.0751779\ttraining's auc: 0.961553\tvalid_1's binary_logloss: 0.130443\tvalid_1's auc: 0.794069\n",
      "[13]\ttraining's binary_logloss: 0.0720615\ttraining's auc: 0.969995\tvalid_1's binary_logloss: 0.130507\tvalid_1's auc: 0.789675\n",
      "[14]\ttraining's binary_logloss: 0.0692159\ttraining's auc: 0.979018\tvalid_1's binary_logloss: 0.130253\tvalid_1's auc: 0.790191\n",
      "[15]\ttraining's binary_logloss: 0.066615\ttraining's auc: 0.984805\tvalid_1's binary_logloss: 0.130365\tvalid_1's auc: 0.786288\n",
      "[16]\ttraining's binary_logloss: 0.0639716\ttraining's auc: 0.986867\tvalid_1's binary_logloss: 0.129717\tvalid_1's auc: 0.786484\n",
      "[17]\ttraining's binary_logloss: 0.061379\ttraining's auc: 0.990912\tvalid_1's binary_logloss: 0.129916\tvalid_1's auc: 0.785257\n",
      "[18]\ttraining's binary_logloss: 0.0590908\ttraining's auc: 0.993059\tvalid_1's binary_logloss: 0.129479\tvalid_1's auc: 0.787171\n",
      "[19]\ttraining's binary_logloss: 0.0569663\ttraining's auc: 0.993561\tvalid_1's binary_logloss: 0.129562\tvalid_1's auc: 0.786079\n",
      "[20]\ttraining's binary_logloss: 0.0548489\ttraining's auc: 0.996005\tvalid_1's binary_logloss: 0.12995\tvalid_1's auc: 0.780114\n",
      "[21]\ttraining's binary_logloss: 0.052841\ttraining's auc: 0.996595\tvalid_1's binary_logloss: 0.129683\tvalid_1's auc: 0.780138\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's binary_logloss: 0.0779005\ttraining's auc: 0.956037\tvalid_1's binary_logloss: 0.131166\tvalid_1's auc: 0.79418\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "fi = []\n",
    "test_probs = []\n",
    "i = 0\n",
    "for train_idx, valid_idx in model_selection.KFold(n_splits=10, shuffle=True).split(X_train):\n",
    "    i += 1\n",
    "    Xt = X_train.iloc[train_idx]\n",
    "    yt = y_train.loc[X_train.index].iloc[train_idx]\n",
    "\n",
    "    Xv = X_train.iloc[valid_idx]\n",
    "    yv = y_train.loc[X_train.index].iloc[valid_idx]\n",
    "\n",
    "    learner = LGBMClassifier(n_estimators=10000)\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)])\n",
    "    \n",
    "    test_probs.append(pd.Series(learner.predict_proba(X_test)[:, -1],\n",
    "                                index=X_test.index, name=\"fold_\" + str(i)))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "\n",
    "test_probs = pd.concat(test_probs, axis=1).mean(axis=1)\n",
    "test_probs.index.name=\"USER_ID\"\n",
    "test_probs.name=\"SCORE\"\n",
    "test_probs.to_csv(\"results.csv\", header=True)\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USER_ID\n",
       "0        0.016390\n",
       "1        0.016288\n",
       "2        0.016346\n",
       "3        0.017179\n",
       "4        0.060283\n",
       "5        0.016700\n",
       "6        0.016466\n",
       "7        0.016495\n",
       "8        0.016402\n",
       "9        0.016654\n",
       "10       0.016393\n",
       "11       0.017743\n",
       "12       0.017512\n",
       "13       0.018021\n",
       "14       0.016706\n",
       "15       0.016259\n",
       "16       0.016644\n",
       "17       0.016322\n",
       "18       0.019225\n",
       "19       0.017767\n",
       "20       0.016341\n",
       "21       0.016759\n",
       "22       0.192044\n",
       "23       0.016429\n",
       "24       0.016685\n",
       "25       0.016346\n",
       "26       0.016345\n",
       "27       0.016733\n",
       "28       0.019751\n",
       "29       0.016660\n",
       "           ...   \n",
       "11646    0.026840\n",
       "11647    0.016533\n",
       "11648    0.082940\n",
       "11649    0.035929\n",
       "11650    0.016671\n",
       "11651    0.047319\n",
       "11652    0.016353\n",
       "11653    0.017633\n",
       "11654    0.018443\n",
       "11655    0.020177\n",
       "11656    0.061017\n",
       "11657    0.022870\n",
       "11658    0.016485\n",
       "11659    0.016647\n",
       "11660    0.025143\n",
       "11661    0.016319\n",
       "11662    0.017336\n",
       "11663    0.033199\n",
       "11664    0.016329\n",
       "11665    0.016662\n",
       "11666    0.017311\n",
       "11667    0.016540\n",
       "11668    0.029188\n",
       "11669    0.025946\n",
       "11670    0.016600\n",
       "11671    0.141936\n",
       "11672    0.016628\n",
       "11673    0.016378\n",
       "11674    0.016319\n",
       "11675    0.016237\n",
       "Name: SCORE, Length: 11676, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
